% by Mirella M. Moro; version: January/18/2012 @ 04:16pm%
% -- 01/18/2012: more discussion on SBBD + kdmile; overall revision
% -- 09/03/2010: bib file with names for proceedings and journals; cls with shrinked {received}
% -- 08/27/2010: appendix, table example, more explanation within comments, editors' data

\documentclass[kdmile,a4paper]{kdmile} % NOTE: kdmile is published on A4 paper
\usepackage{graphicx,url}  % for using figures and url format
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}   % avoids warnings such as "LaTeX Font Warning: Font shape 'OMS/cmtt/m/n' undefined"
\usepackage[brazilian]{babel}
\usepackage[usenames,dvipsnames]{color}
\usepackage{soul,xcolor}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[normalem]{ulem}

%\usepackage{cite} % NOTE: do **not** include this package because it conflicts with kdmile.bst

% Standard definitions
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newdef{definition}[theorem]{Definition}
\newdef{remark}[theorem]{Remark}

% New environment definition
\newenvironment{latexcode}
{\ttfamily\vspace{0.1in}\setlength{\parindent}{18pt}}
{\vspace{0.1in}}

% Configurações gráficas
\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg} % Preferência por .eps (maior resolução)
\graphicspath{{fig/}}                           % Pasta padrão para as figuras

% Traduções ref. packaged 'algorithm[ic]'
\floatname{algorithm}{Algoritmo} % termo utilizado na referenciação
\renewcommand{\algorithmicrequire}{\textbf{Requer:}}
\renewcommand{\algorithmicensure}{\textbf{Assegurar-se:}}
\renewcommand{\algorithmicend}{\textbf{fim}}
\renewcommand{\algorithmicif}{\textbf{se}}
\renewcommand{\algorithmicthen}{\textbf{então}}
\renewcommand{\algorithmicelse}{\textbf{senão}}
\renewcommand{\algorithmicelsif}{\algorithmicelse\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicforall}{\textbf{para todo}}
\renewcommand{\algorithmicdo}{\textbf{faça}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{enquanto}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{loop}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{repita}}
\renewcommand{\algorithmicuntil}{\textbf{até}}
\renewcommand{\algorithmicprint}{\textbf{imprima}}
\renewcommand{\algorithmicreturn}{\textbf{retorne}}
\renewcommand{\algorithmictrue}{\textbf{verdadeiro}}
\renewcommand{\algorithmicfalse}{\textbf{falso}}
\renewcommand{\algorithmicand}{\textbf{e}}
\renewcommand{\algorithmicor}{\textbf{ou}}

% Eventos salientes
\newcommand{\tmon}{$alarme_{ON}$}
\newcommand{\tmoff}{$alarme_{OFF}$}
\newcommand{\muon}{$musica_{ON}$}
\newcommand{\muoff}{$musica_{OFF}$}
\newcommand{\lion}{$luz_{ON}$}
\newcommand{\lioff}{$luz_{OFF}$}
\newcommand{\beon}{$sino_{ON}$}

% Termos/conceitos
\newcommand{\qt}{Q(s_t, a_t)}
\newcommand{\qtt}{Q(s_{t+1}, a)}
\newcommand{\imrl}{IMRL}
\newcommand{\mi}{motivação intrínseca}
\newcommand{\epS}{\xi_{p}}
\newcommand{\ep}{$\epS$}
\newcommand{\pilhaS}{\Pi}
\newcommand{\pilha}{$\pilhaS$}
\newcommand{\ppS}{\phi}
\newcommand{\pps}{$\ppS$}
\newcommand{\ppc}[1]{$\ppS_{#1}$}
\newcommand{\OsetS}{\Phi}
\newcommand{\Oset}{$\OsetS$}
\newcommand{\attS}{a_{t+1}}
\newcommand{\att}{$\attS$}
\newcommand{\qbS}{Q^B}
\newcommand{\qb}{$\qbS$}
\newcommand{\td}{\emph{top-down}}
\newcommand{\bu}{\emph{bottom-up}}

%% Added para versão revisada
\newcommand{\alg}{A-EIP$^3$}

% := e =:
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

\newcommand{\quinhentosmil}{$5\times10^5$}

% Comando para espaçamento vertical no ambiente algorithm
\newcommand{\GIMMESPACE}{\vspace{3.17 mm}}

% Para marcar palavras que precisam ser traduzidas
% \newcommand{\tr}[1]{\colorbox{MidnightBlue}{\textcolor{White}{\textbf{#1}}}}
\newcommand{\tr}[1]{\textcolor{MidnightBlue}{\textbf{#1}}}

% Para marcar partes do algoritmo original que foram modificadas
\sethlcolor{Green}
\newcommand{\grifar}[1]{\dotuline{#1}}
% \newcommand{\grifar}[1]{\hl{#1}}
% \newcommand{\grifar}[1]{\dashuline{#1}}
% \newcommand{\grifar}[1]{\uwave{#1}}

% Substituir pelo título quando estiver definido
\newcommand{\titulo}{Estratégias para Aplicação de\\ Políticas Parciais com Motivação Intrínseca}

%% Melhores comentários para o pacote algorithm
\renewcommand{\algorithmiccomment}[1]{#1}

% ALL FIELDS UNTIL BEGIN{document} ARE MANDATORY


% Includes headers with simplified name of the authors and article title
\markboth{R. L. Beirigo and V. Freire and A. H. R. Costa}
{\titulo}
%  -> \markboth{}{}
%         takes 2 arguments
%         ex: \markboth{A.Plastino}{Any article title}


% Title of the article
\title{\titulo}


% List of authors
%IF THERE ARE TWO or more institutions, please use:
%\author{Name of Author1\inst{1}, Name of Author2\inst{2}, Name of Author3\inst{2}}
\author{R. L. Beirigo and V. Freire and A. H. R. Costa}

%Affiliation and email
\institute{Escola Politécnica da Universidade de São Paulo, Brazil \\
  \email{\{rafaelbeirigo, valdinei.freire, anna.reali\}@usp.br}
}

% Article abstract - it should be from 100 to 300 words
\begin{abstract}
  As técnicas de Aprendizado por Reforço permitem a solução de um
  problema através da escolha de ações que maximizem valores de
  recompensas recebidas que refletem a qualidade das ações tomadas
  pelo agente em um processo de tentativa e erro. Em problemas com
  estrutura hierárquica, a solução final depende do encadeamento de
  soluções para subproblemas aí presentes, sendo frequente a
  repetição de subproblemas nesse encadeamento. Nesses casos, a
  utilização de políticas parciais permite o aprendizado e
  armazenamento das soluções individuais para cada subproblema, que
  podem então ser utilizadas múltiplas vezes na composição de uma
   solução completa para o problema final, acelerando o
  aprendizado. Apesar de vantajosa, a utilização de políticas parciais
  necessita de definições por parte do projetista, o que representa
  uma sobrecarga. Para contornar esse problema, foram propostas
  técnicas de descoberta automática de políticas parciais, dentre as
  quais a utilização de motivação intrínseca se destaca por permitir
  ao agente aprender soluções de subproblemas úteis na solução do
  problema final sem a necessidade de se definir manualmente novas
  recompensas para esses subproblemas individualmente. Apesar de
  promissora, essa proposta utiliza um conjunto de componentes de
  aprendizado que ainda carece de investigação aprofundada acerca dos
  impactos individual e coletivo de cada componente, notadamente a
  aplicação das políticas parciais durante o aprendizado. Neste
  trabalho são investigadas estratégias de aplicação de políticas
  parciais durante o Aprendizado por Reforço com Motivação
  Intrínseca. Duas estratégias são propostas e o desempenho de cada
  estratégia é avaliado experimentalmente. Com base nesta avaliação,
  uma nova estratégia é proposta, a qual utiliza as alternativas que
  apresentaram o melhor desempenho na avaliação experimental.
\end{abstract}

% ACM Computing Classification System categories
\category{I.2.6}{Artificial Intelligence}{Learning} 

% Categories and Descriptors are available at the 1998 ACM Computing Classification System
% http://www.acm.org/about/class/1998/
%  -> \category{}{}{}
%         takes 3 arguments for the Computing Reviews Classification Scheme.
%         ex: \category{D.3.3}{Programming Languages}{Language Constructs and Features}
%                   [data types and structures]
%                   the last argument, in square brackets, is optional.

% Article keywords
\keywords{aprendizado por reforço, descoberta de políticas parciais, motivação intrínseca}
%  -> \keywords{} (in alphabetical order \keywords{document processing, sequences,
%                      string searching, subsequences, substrings})

% THE ARTICLE BEGINS
\begin{document}

% This is optional:
\begin{bottomstuff}
% TODO: preencher adequadamente
% similar to \thanks
% for authors' addresses; research/grant statements
\end{bottomstuff}

\maketitle

\section{Introdução}
Problemas nos quais o agente deve tomar uma série de decisões
sequenciais podem ser resolvidos através de técnicas de Aprendizado
por Reforço \cite{rlczaba} (AR), em que o agente recebe um reforço
numérico indicando a qualidade de cada ação tomada.  Esse processo de
aprendizado ocorre por tentativa e erro, no qual o agente alterna
entre a \emph{aplicação} do conhecimento adquirido até o momento para
selecionar a melhor ação e a \emph{exploração} do ambiente, onde o
agente experimenta novas ações, aumentando o seu conhecimento sobre o
ambiente e o problema.  Apesar do sucesso apresentado na aplicação de
técnicas AR a problemas reais
\cite{tesauro1995tdgammon,barto1996elevator,scardua2002optimal,da2006inverse,abbeel2007helicopter},
elas podem apresentar indesejada lentidão na obtenção das soluções
\cite{barto2003hrl}.
 
Uma forma de mitigar essa lentidão é explorar características da
\emph{representação} do problema original que permitam a representação
de subproblemas.  Possibilitar que um problema de AR originalmente com
representação monolítica seja representado de forma modular permite o
uso de políticas parciais \cite{option1999}.  Na nova representação,
são determinados subproblemas componentes do problema original,
permitindo o aprendizado simultâneo da solução para o problema
original e seus subproblemas.  Quando o problema original possui uma
estrutura \emph{hierárquica}, é comum o agente ter de resolver mais de
uma vez o mesmo subproblema em momentos diferentes.  Nesses casos, a
utilização de políticas parciais apresenta a vantagem de acelerar o
aprendizado por eliminar a necessidade de se aprender a solução para o
mesmo subproblema mais de uma vez, permitindo o reuso do conhecimento
já adquirido na forma de políticas parciais \cite{option1999} e dessa
forma acelerando o aprendizado.

Entretanto, para que as políticas parciais possam ser utilizadas, há a
necessidade da definição de elementos cuja identificação pode não ser
imediata à apreciação do problema, podendo causar sobrecarga em tempo
de projeto.  Para contornar esse revés, são estudadas técnicas de
descoberta automática de políticas parciais
\cite{stolle2002learning,bakker2004hierarchical,mcgovern2001automatic,imrl2004},
em que a definição das mesmas fica a cargo de um algoritmo responsável
por identificar subproblemas e criar políticas parciais para
resolvê-los.  Das diversas técnicas empregadas, pode ser destacada a
utilização de \emph{motivação intrínseca} em AR \cite{imrl2004}, em
que o agente recebe recompensas adicionais através de um mecanismo
interno associado a variações de características do ambiente que lhe
despertam \emph{marcado interesse}.  Nessas técnicas, o agente é
codificado de forma a associar subproblemas a variações significativas
de atributos selecionados na descrição fatorada de cada estado do
problema, reduzindo o trabalho de definição das políticas parciais à
determinação dos atributos relevantes para a identificação dos
subproblemas.

Essa técnica foi aplicada com sucesso na solução de um problema com
forte estrutura hierárquica \cite{imrl2004}, através da proposta de um
algoritmo que utiliza em conjunto múltiplas técnicas de AR. Apesar da
apresentação detalhada do papel da motivação intrínseca no
aprendizado, não há uma investigação mais aprofundada dos diversos
componentes de aprendizado envolvidos algoritmo, notadamente a
estratégia de aplicação das políticas parciais aprendidas. Neste
trabalho, investigamos essa questão, apresentando estratégias para a
aplicação das políticas parciais aprendidas juntamente com a avaliação
experimental de cada uma delas.  A primeira contribuição refere-se ao
armazenamento da aplicação de políticas parciais através de uma
estrutura de dados em forma de pilha, que armazena incrementalmente as
políticas parciais utilizadas pelo agente.  Para o processo de
terminação das políticas parciais e consequente remoção da pilha,
investiga-se o papel da exploração nesse processo.  Os resultados
experimentais obtidos sugerem a aplicabilidade das técnicas propostas
para o aprendizado de políticas parciais com a utilização de \mi.

Este artigo está organizado da seguinte forma.  A Seção
\ref{sec:fundamentosteoricos} apresenta os conceitos fundamentais
envolvidos na modelagem e solução do problema de AR, incluindo o
algoritmo do trabalho-base que utiliza o conceito de motivação
intrínseca.  A proposta apresentada no presente trabalho é descrita na
Seção \ref{sec:proposta}.  A avaliação experimental da proposta e os
resultados alcançados são descritos na Seção
\ref{sec:avaliacaoexperimental}. Finalmente, a Seção
\ref{sec:conclusao} apresenta as conclusões e direcionamentos futuros

\section{Fundamentos Teóricos}\label{sec:fundamentosteoricos}
Problemas de decisão sequencial são usualmente representados por um
\emph{Processo Markoviano de Decisão} (PMD) \cite{puterman2009markov},
definido pela quádrupla $\langle S, A, P, R \rangle$, em que $S$ e $A$
são conjuntos finitos de estados e ações respectivamente, sendo $R
\colon S \to \Re$ representa a recompensa \emph{extrínseca} $r^e$
recebida ao alcançar um estado $s \in S$ e $P \colon S \times A \times
S \to [0,1]$ a probabilidade de se alcançar $s_{t+1} \in S$ a partir
de $s \in S$ ao se executar a ação $a \in A$.  Uma política $\pi
\colon S \to A$ associa uma ação a ser executada em um estado e a
função $Q \colon S \times A \to \Re$ fornece o valor esperado de
recompensas futuras ao se executar a ação $a$ no estado $s$, e pode
ser obtida iterativamente pelo Q-Learning \cite{watkins1989learning}
por
%
$\qt \leftarrow (1 - \alpha)\qt + \alpha (r_{t+1} + \gamma \max_a\qtt)$,
%
onde $\alpha \in\ ]0,1]$ e $\gamma \in [0,1]$ correspondem à taxa de
aprendizado e ao fator de desconto futuro, respectivamente e $s_{t+1}$
ao estado alcançado após executar-se a ação $a_t$ em $s_t$.  A solução
para o PMD é a política ótima $\pi^*$, que pode ser obtida por
$\pi^*(s) = \arg\max_a Q^*(s, a), \forall s \in S$.  No Q-Learning as
ações são selecionadas pela estratégia $\epsilon$-gulosa, que
recomenda ações aleatórias e ótimas com probabilidades $\epsilon \in
[0,1]$ e $1 - \epsilon$, respectivamente.  Dessa forma, o agente
aplica o conhecimento adquirido sobre o problema, aprimorando-o
através de exploração.

Por ser realizado através de tentativa e erro esse processo é
geralmente lento e pode ser acelerado pela utilização de políticas
parciais \cite{option1999} \pps$\ \defeq\ \langle \mathcal{I},\ \pi,
\beta \rangle$, em que $\mathcal{I} \subseteq S$ indica os estados em
que a política parcial \pps\ pode ser iniciada, $\beta \colon S \to
[0,1]$ a probabilidade de terminar \pps\ em um estado $s \in S$ e $\pi
\colon S \to \OsetS$, associa ações primitivas ou políticas parciais a
serem executas, onde \Oset\ corresponde ao conjunto de todas as ações
primitivas e políticas parciais.  A obtenção da solução do problema
original é obtida pela divisão em subproblemas para os quais são
aprendidas políticas parciais que podem aplicar umas às outras, desta
forma conferindo a esta técnica um forte caráter hierárquico.

\setlength\intextsep{1pt}
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{algorithm}[H]
        \caption{\imrl}
        \label{alg:imrl}
        \algsetup{indent=1em}
        \small
        \begin{algorithmic}[1]
          \newcommand{\sd}{s_{t+1}}
          \newcommand{\bo}[1]{$\beta^o(#1)$}
          \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
          \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
          \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
          \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
          \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
          \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
          \newcommand{\termoA}{\bo{\sd}$v$}
          \newcommand{\termoB}{[\ubo{\sd}][\maos]}
          \newcommand{\termoC}{\bo{x}$v$}
          \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
          \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

          \REQUIRE{Inicializar $s_t,\ a_t,\ $\ppc{t}$,\ r^e_t,\ r^i_t$}
          \REPEAT
          \STATE\COMMENT{Obter $\sd$, $r^e_{t+1}$ pela transição $s_t,\ a_t \rightarrow s_{t+1}$}
          \STATE\COMMENT{Tratar Ocorrência de Evento Saliente}
          \STATE\COMMENT{Atualizar Modelo (Políticas Parciais)}
          \STATE\COMMENT{Atualizar Função-Valor (Q-Learning)}
          \STATE\COMMENT{Atualizar Módulo de Planejamento}
          \STATE\COMMENT{Atualizar Funções-Valor (Políticas Parciais)}
          \STATE\COMMENT{Obter $a_{t+1}$ (estratégia $\epsilon$-gulosa)}
          \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}
          \UNTIL{condição de parada}
        \end{algorithmic}
      \end{algorithm}
    \end{center}
  \end{minipage}
\end{wrapfigure}
A obtenção de políticas parciais pode ser realizada de forma
automática simultaneamente à solução do problema original através da
utilização de motivação intrínseca para identificação de subproblemas
e correspondente aprendizado de políticas parciais.  O Agoritmo
\ref{alg:imrl}, proposto em \cite{imrl2004} (IMRL), ao lado,
implementa o conceito de \emph{recompensas intrínsecas}, representadas
por $r^i$, recebidas quando ocorrem variações significativas de
determinados atributos presentes da descrição fatorada do estado,
sendo denominados \emph{atributos salientes} e a ocorrência da
variação denominada \emph{evento saliente}, sendo o conceito utilizado
para a obtenção, atualização e aplicação do modelo e função valor para
a obtenção das políticas parciais.

\section{Proposta}\label{sec:proposta}
Políticas parciais podem ser obtidas automaticamente simultaneamente
ao aprendizado através do algoritmo apresentado em \cite{imrl2004},
que utiliza motivação intrínseca para detectar a presença de
subproblemas e associar a cada um deles uma política parcial para
resolvê-lo.  Entretanto, o algoritmo proposto concentra-se nos
aspectos relacionados à \emph{obtenção} da políticas parciais.  Neste
trabalho, são investigados dois aspectos da \emph{aplicação} das
políticas parciais: exploração interna e método de terminação, através
da adição de duas funcionalidades ao algoritmo original.

\setlength\intextsep{1pt}
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{algorithm}[H]
        \caption{Remoção de políticas parciais de \pilha}
        \label{alg:removedapilha}
        \algsetup{indent=1em}
        \small
        \begin{algorithmic}[1]
          \newcommand{\sd}{s_{t+1}}
          \newcommand{\bo}[1]{$\beta^o(#1)$}
          \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
          \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
          \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
          \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
          \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
          \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
          \newcommand{\termoA}{\bo{\sd}$v$}
          \newcommand{\termoB}{[\ubo{\sd}][\maos]}
          \newcommand{\termoC}{\bo{x}$v$}
          \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
          \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

          \REQUIRE{Realizar antes de cada obtenção de $a_{t+1}$}
          \FOR{\ppc{i}~$\in$~$[$\pilha(base)~$..$~\pilha(topo)$]$}
            \IF{$\beta^{\ppS_i}(s_{t+1})=1$}
              \FOR{\ppc{j}~$\in$~$[$\ppc{i}~$..$~\pilha(topo)$]$}
                \STATE\COMMENT{Remover \ppc{j} de \pilha}
              \ENDFOR
            \ENDIF
          \ENDFOR
        \end{algorithmic}
      \end{algorithm}
    \end{center}
  \end{minipage}
\end{wrapfigure}
No momento da obtenção da próxima ação, feita de forma aleatória ou
gulosa, caso a próxima ação selecionada seja uma política parcial
\ppc{1}, esta é adicionada ao topo da pilha \pilha.  Dessa forma, um
agente que inicia com uma pilha vazia $\pilhaS=[\ ]$ e obtém como
próxima ação a política parcial \ppc{1} resultará com
$\pilhaS=[$\ppc{1}$]$.  Caso a aplicação de \ppc{1} sugira a execução
de uma nova política parcial \ppc{2}, essa será então empilhada,
resultando em $\pilhaS=[$\ppc{1}$,\ $\ppc{2}$]$ e assim sucessivamente
para novas políticas parciais.  Uma política parcial é removida da
pilha no momento em que seu objetivo é atingido pelo agente, ou seja,
o evento saliente utilizado para criá-la é disparado.  Como uma
política parcial \pps\ pode invocar outras de modo a atingir seu
objetivo, \pps\ é removida da pilha juntamente com as políticas
parciais invocadas por ela.  A estratégia de remoção testa as
políticas parciais para remoção partindo da base da pilha em direção
ao topo, ou seja, prioriza a remoção das políticas parciais mais
anteriormente adicionadas, como mostrado no Algoritmo
\ref{alg:removedapilha}.

Propõe-se também um mecanismo de \emph{exploração interna} à política
parcial, elaborada para atenuar o efeito de \emph{rigidez}
potencialmente imposto pela política parcial, em que o agente fica
preso a uma sequência de ações.  Isso foi feito porque, apesar de a
estratégia gulosa ser a melhor no caso de a política parcial ser ótima
para o subproblema sendo resolvido, ela implica em uma restrição que
potencialmente atrapalharia o desempenho de aprendizado do agente nos
passos iniciais, em que as políticas parciais estão em fase inicial de
aprendizado.  De forma a contornar esse problema, propõe-se a
aplicação da estratégia $\epsilon$-gulosa para a escolha da próxima
ação a ser executada durante a aplicação das política parciais.

\setlength\intextsep{1pt}
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{algorithm}[H]
        \caption{Exploração interna à política parcial}
        \label{alg:explorainternamente}
        \algsetup{indent=1em}
        \small
        \begin{algorithmic}[1]
          \newcommand{\sd}{s_{t+1}}
          \newcommand{\bo}[1]{$\beta^o(#1)$}
          \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
          \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
          \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
          \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
          \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
          \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
          \newcommand{\termoA}{\bo{\sd}$v$}
          \newcommand{\termoB}{[\ubo{\sd}][\maos]}
          \newcommand{\termoC}{\bo{x}$v$}
          \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
          \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

          \REQUIRE{\pps\ em aplicação pelo agente, \ep~$\in [0,1]$}
          \IF{Aleatório([0,1])~$<$~\ep}
            \STATE\COMMENT{$a_{t+1}$~$\leftarrow$~SeleçãoGulosa(\pps, $s_t$)}
          \ELSE
            \STATE\COMMENT{$a_{t+1}$~$\leftarrow$~SeleçãoAleatória(\Oset, $s_t$)}
          \ENDIF
        \end{algorithmic}
      \end{algorithm}
    \end{center}
  \end{minipage}
\end{wrapfigure}
Para isso, introduziu-se o parâmetro $\epS \in [0,1]$, que indica o
percentual de exploração utilizado na aplicação de uma política
parcial.  No momento de selecionar a próxima ação a ser executada,
caso o agente esteja executando uma política parcial \pps\, é sorteado
um valor no intervalo $[0,1]$; caso esse valor seja maior que \ep\, o
agente aplica a política parcial sendo seguida de forma gulosa e, em
caso contrário, o agente realiza exploração, ou seja, executa uma ação
aleatória dentre as possíveis para o estado atual, conforme
apresentado no Algoritmo \ref{alg:explorainternamente}.  Caso o agente
não esteja seguindo uma política parcial, a seleção da próxima ação é
feita da forma usual, aplicando a estratégia $\epsilon$-gulosa no
conjunto das ações primitivas e políticas parciais possíveis de serem
executadas no estado em que o agente se encontra.

\section{Avaliação Experimental}\label{sec:avaliacaoexperimental}
\setlength\intextsep{1pt}
\begin{wrapfigure}[14]{r}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \newcommand{\fppt}{\fontsize{.295cm}{1em}\selectfont}
    \fppt
    \begin{center}
      % BEGIN RECEIVE ORGTBL thenameofthetable
      \begin{tabular}{llll}
        \hline
        Objeto & Ação & Evento & Mov. \\
        &  & saliente &  \\
        \hline
        alarme & - & \tmon\ & - \\
        & - & \tmoff\ & - \\
        botão A & pressionar & \muon\ & - \\
        & empurrar & - & V \\
        botão B & pressionar & \muoff\ & - \\
        & empurrar & - & V \\
        bola & chutar & - & A \\
        interruptor & pressionar & \lion & - \\
        & pressionar & \lioff & - \\
        sino & - & \beon\ & V \\
        \hline
      \end{tabular}
      % END RECEIVE ORGTBL thenameofthetable
    \end{center}
    \caption{\fppt \emph{Playroom}: objetos, ações e eventos salientes}
    \label{fig:obj}
  \end{minipage}
\end{wrapfigure}
O domínio utilizado na avaliação experimental foi o \emph{Playroom}
\cite{imrl2004}, em que o agente se econtra em um reticulado
$5\times5$ e deve descobrir como ativar um \emph{alarme} interagindo
com objetos aí presentes. O agente é composto de um \emph{olho}, uma
\emph{mão} e um \emph{alvo} e possui sempre disponíveis as seguintes
ações: apontar \emph{olho} para a célula onde se encontra a \emph{mão}
ou para a célula onde se encontra o \emph{alvo}; mover a \emph{mão} ou
o \emph{alvo} para a célula atualmente apontada pelo \emph{olho}.

\setlength\intextsep{0pt}
\begin{wrapfigure}[11]{r}{0.3\textwidth}
  \begin{minipage}{0.3\textwidth}
    \begin{center}
      \begin{figure}[H]
        \includegraphics[width=\textwidth]{EfeitoMI}
        \tiny
        \caption{Efeito da Motivação Intrínseca no Aprendizado}
        \label{fig:mi}
      \end{figure}
    \end{center}
  \end{minipage}
\end{wrapfigure}
Na Figura \ref{fig:obj} estão relacionados os objetos aí presentes,
juntamente com as ações e o evento
saliente que disparam.  Essas ações ficam disponíveis somente quando o
\emph{olho} e a \emph{mão} estão sobre a célula em que se encontra o
objeto.  Na coluna \emph{Mov.}, objetos marcados com \emph{A} movem-se
para a posição atual do \emph{alvo} e as marcadas com \emph{V} para
uma célula vizinha aleatória ao serem ativados.  Caso a \emph{bola}
seja chutada com o \emph{alvo} sobre o \emph{sino}, este é ativado,
disparando o evento \beon.  O \emph{alarme} dispara quando, com a
música ligada e a luz desligada, ocorre \beon\ e é desativado quando o
estado da luz ou da música é modificado; o agente somente consegue
diferenciar os botões \emph{A} e \emph{B} e executar as ações
associadas a eles se a luz estiver ligada.

Para avaliar a técnica proposta, foram realizados dois experimentos,
sendo que em um deles o módulo de obtenção de políticas parciais a
partir de \mi\ foi desativado, ou seja, o agente somente receberia
recompensas extrínsecas, sem a obtenção de políticas parciais.  Foram
realizados 100 experimentos, de \quinhentosmil\ passos cada, com o
agente recebendo uma recompensa extrínseca para ocorrência de \tmon\
no valor de 10 e recompensas intrínsecas para os demais eventos
salientes no valor de 1, utilizando os parâmetros $\alpha$=$0.1,\
\gamma$=$0.99,\ \epsilon$=$0.1,\ \tau$=$0.5$ e \ep=$0.3$.  Os
resultados são apresentados no gráfico da Figura \ref{fig:mi}, que
apresenta a média de passos necessários para ativar \tmon\ por
quantidade de ativação. Os resultados aí presentes sugerem maior
eficiência de aprendizado por parte do agente que utiliza \mi\, pois
este diminui o tempo necessário para a solução do problema com uma
maior rapidez em relação ao agente que utiliza somente motivação
extrínseca.

\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{figure}[H]
        \begin{center}
          \includegraphics[width=0.95\textwidth]{r_i-2014-07-12_07-15-03-Teri}
          \caption{Recebimento de recompensas intrínsecas por evento saliente por passo de aprendizado}
          \label{fig:ri}
        \end{center}
      \end{figure}
    \end{center}
  \end{minipage}
\end{wrapfigure}
No gráfico da Figura \ref{fig:ri} são apresentados os valores de
recompensa intrínseca recebida por passo do experimento, evidenciando
algumas características do aprendizado utilizando \mi : o agente
inicialmente procede à execução das tarefas mais simples, como
ligar/desligar a luz, o que pode ser visto pela frequência e
prematuridade com que o agente recebe recompensas intrínsecas para
essas tarefas. Pode-se perceber também uma variação dos valores
recebidos associados ao evento saliente com a repetição dos mesmos: há
uma tendência de \emph{diminuição} dos valores recebidos para eventos
salientes repetidos, mostrando a capacidade da técnica de priorizar a
realização de tarefas menos conhecidas.

Para avaliar o impacto de \ep\ no aprendizado, logo, do percentual de
exploração interna à política parcial, foram realizados 100
experimentos de $5 \times 10^5$ passos cada nos quais os valores
de~\ep\ variaram em $\epS \in \{0,0; 0,2; 0,4; 0,6; 0,8; 1,0\}$, com
valores dos demais parâmetros idênticos aos listados acima.  O
desempenho desses robôs foi comparado com um robô aprendendo pelo
Q-Learning e um robô executando ações aleatórias, sem nenhuma
estratégia e sem realizar aprendizado.

\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{figure}[H]
        \begin{center}
          \includegraphics[width=0.95\textwidth]{toyonxstep-mediaacum-epsopt-parcial-00-02-04-06-08-10}
          \caption{Avaliação da estratégia de pilhas de políticas parciais
              para exploração interna com diferentes valores de \ep,
              \emph{versus} Q-Learning e comportamento aleatório.  O gráfico
              mostra a quantidade média acumulada de vezes que cada agente
              conseguiu resolver o problema.}
          \label{fig:ep}
        \end{center}
      \end{figure}
    \end{center}
  \end{minipage}
\end{wrapfigure}
Os resultados obtidos são apresentados no gráfico da
Figura~\ref{fig:ep}.  No gráfico foi plotada a média acumulada da
quantidade de vezes que cada robô conseguiu resolver o problema por
passo do experimento.  No primeiro passo o valor é 0 para todos os
robôs, pois com um passo nenhum robô é capaz de disparar \tmon; após
os quinhentos passos do aprendizado, o valor corresponde à média da
quantidade \emph{total} de vezes que o respectivo robô disparou \tmon.
Assim, por exemplo, o robô que utilizou o \alg\ com $\epS=0,6$
conseguiu disparar \tmon\ $588,13$ nos quinhentos mil passos, sendo a
média calculada para as 100 repetições, o robô que aprendeu com o
Q-Learning disparou uma média de $423,19$ vezes e o robô que executou
ações aleatórias, $95,98$.

Procedendo à análise dos dados presentes no gráfico, inicialmente
considere-se o robô que aprendeu com o \alg, utilizando os valores de
$\epS \in \{0,2; 0,4; 0,6; 0,8\}$.  Observando-se o gráfico, pode-se
perceber que um \emph{maior} valor de \ep\ parece obter um
\emph{melhor} desempenho, ou seja, o desempenho dos respectivos robôs
é proporcional ao valor de \ep\ utilizado, sendo o melhor desempenho
do conjunto apresentado pelo robô que utilizou $\epS=0,8$.  Outra
maneira de ler os dados é a de que quanto maior a exploração interna à
política parcial, melhor o desempenho do robô, dado que essa
exploração é controlada pelo parâmetro \ep.

Esse resultado pode parecer contra-intuitivo à primeira vista, dado
que a função da política parcial é \emph{auxiliar} o robô a resolver
os subproblemas, dessa forma esperando-se que quanto mais o robô se
ativer às recomendações da política parcial, melhor será seu
desempenho.

Entretanto, é importante considerar que as políticas parciais são
\emph{descobertas}, \emph{criadas} e \emph{aprendidas} no \alg.  Dessa
forma, é de se esperar que a qualidade das mesmas somente esteja
relativamente \emph{satisfatória} após algum tempo de aprendizado.
Assim, o auxílio da exploração interna à política parcial no
desempenho dos robôs pode ser explicada pela importância que a
exploração interna tem \emph{no próprio aprendizado da política
    parcial}, ou seja, os resultados sugerem a necessidade de o robô
realizar exploração no aprendizado das políticas parciais.

A hipótese de que \emph{a exploração interna à política parcial
    favorece o aprendizado} é corroborada pelo resultado
correspondente ao robô que utilizou o valor de $\epS=0,0$, ou seja, o
robô que \emph{não realizou exploração interna às políticas parciais},
e que apresentou o desempenho menos favorável do conjunto.  Esse
resultado apoia, portanto, a hipótese de que o aprendizado das
políticas parciais pode ser auxiliado pela exploração interna.

Comparando-se os resultados apresentados pelo robô que aprendeu com o
\alg\ utilizando $\epS=0,8$ e pelo robô que aprendeu com o Q-Learning,
os dados presentes no gráfico sugerem uma desvantagem de desempenho
final por parte do robô que aprendeu com o Q-Learning, corroborando os
dados apresentados anteriormente, referentes à Figura~\ref{fig:mi}.
Entretanto, o desempenho inicial correspondente ao Q-Learning supera o
do \alg, o que é revertido somente pouco antes da metade do
experimento.  Esse resultado pode ser explicado pela sobrecarga
representada pelo aprendizado das políticas parciais relacionada ao
\alg, também corroborando os resultados anteriores apresentados na
discussão relacionada à Figura~\ref{fig:mi}.

Por outro lado, o robô que utilizou $\epS=0,4$ apresentou um
desempenho inferior ao do Q-Learning, o que aponta uma característica
indesejável do \alg: uma dependência em termos da escolha do valor do
parâmetro $\epS$.  Um resultado ainda mais expressivo é o apresentado
pelo robô que utilizou $\epS=0,2$: esse robô apresentou um resultado
\emph{inferior ao robô que executou ações aleatórias}.  Esse resultado
sugere, portanto, uma significativa sensibilidade do \alg\ ao valor de
\ep, sendo que uma má escolha de \ep\ pode comprometer
substancialmente o desempenho de aprendizado.

Assim, os resultados obtidos para o aprendizdo com o \alg\ utilizando
os valores de $\epS \in \{0,2; 0,4; 0,6; 0,8\}$ sugerem a
aplicabilidade da técnica proposta para o aprendizado, inclusive
podendo superar uma técnica de aprendizado tradicional como o
Q-Learning.  Entretanto, a técnica é fortemente dependente do valor
escolhido para $\epS$, sendo que uma má escolha de \ep\ pode
\emph{prejudicar expressivamente} o aprendizado.

O robô que realizou o aprendizado utilizando o valor de $\epS=1,0$
realizou exploração $100\%$ das vezes em que uma política parcial era
aplicada, ou seja, no momento em que o agente iria obter uma ação
através da política parcial sendo aplicada, no lugar era realizado um
sorteio de uma ação aleatória a ser executada ao invés da ação que
seria sugerida pela política parcial.  Esse robô ainda realizou o
aprendizado paralelo, realizado por um módulo similar ao utilizado
pelo Q-Learning, como detalhado na Seção~\ref{sec:imrl}, onde as ações
\emph{primitivas} ainda eram escolhidas segundo uma estratégia
$\epsilon$-gulosa.  Entretanto, devido ao fato de explorar
\emph{sempre} que aplicava uma política parcial, esperava-se que o
desempenho apresentado pelo robô que utilizou $\epS=1,0$ fosse similar
ao robô que realizou ações aleatórias, o que não ocorreu.  Esse
resultado representou uma surpresa na fase experimental, e será
discutido com o auxílio dos resultados apresentados a seguir.

Para verificar o impacto isolado da proposta de empilhamento de
políticas parciais, realizou-se um conjunto de experimentos em que os
robôs que aprenderam com $\epS \in \{0,0; 0,2; 0,4; 0,6; 0,8; 1,0\}$,
mas, nesse novo conjunto de experimentos, \emph{os robôs não
    utilizaram a pilha de políticas parciais}, ou seja, aplicaram as
políticas parciais aprendidas até a terminação, sem, entretanto,
empilhá-las, como proposto neste trabalho.

\setlength\intextsep{0pt}
\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{figure}[H]
        \begin{center}
          \includegraphics[width=0.95\textwidth]{toyonxstep-mediaacum-epsopt-sempilha-parcial-00-02-04-06-08-10}
          \caption{Avaliação do impacto da utilização da estratégia de pilhas.
              O gráfico é análogo ao da Figura~\ref{fig:ep}, com a diferença
              de que este gráfico apresenta o desempenho dos robôs utilizando
              exploração interna sem utilizar empilhamento de políticas
              parciais.}
          \label{fig:epsp}
        \end{center}
      \end{figure}
    \end{center}
  \end{minipage}
\end{wrapfigure}
Os resultados são aprentados na Figura~\ref{fig:epsp}, que apresenta
um gráfico análogo ao da Figura~\ref{fig:ep}, mas considerando os
robôs que não utilizaram a pilha de políticas parciais.  Analisando
primeiramente o desempenho do robô que utilizou $\epS=1,0$, pode-se
perceber que, ao não utilizar a pilha de políticas parciais, o robô,
explorando $100\%$ das vezes em que aplicou as políticas parciais,
\emph{apresentou um resultado} comparável ao do agente que realizou
ações aleatórias, como era esperado que ocorresse e que não ocorreu
quando o robô utilizou o empilhamento de políticas parciais.

Dessa forma, a análise conjunta dos resultados para $\epS=1,0$
\emph{com} e \emph{sem} empilhamento de políticas parciais sugere que
o empilhamento de políticas parciais pode ser prejudicial ao processo
de exploração interna às políticas parciais, ou seja, as propostas
deste trabalho são \emph{conflitantes}.  Uma interpretação alternativa
seria a de que há um \emph{compromisso} entre o \emph{fator de
    exploração} e o \emph{fator de empilhamento}.

Comparando os desempenhos dos robôs que utilizaram a pilha de
políticas parciais e dos robôs que não utilizaram, e considerando a
alta estocasticidade do domínio, \emph{não é possível afirmar que a
    estratégia de empilhamento de políticas parciais apresente um
    impacto significativo no desempenho de aprendizado}, dada a
similaridade dos desempenhos entre os dois conjuntos de robôs, como
pode ser visto pela análise conjunta dos gráficos das
Figuras~\ref{fig:ep}~e~\ref{fig:epsp}.  Entretanto, o desempenho
crescente com o valor de $\epS$ corrobora a hipótese anterior, de que
a exploração interna favorece o aprendizado das políticas parciais.

A investigação experimental do impacto da utilização de empilhamento
de políticas não apresentou resultados favoráveis: os resultados
obtidos sugerem que o empilhamento de políticas parciais não impacta
significativamente no processo de aprendizado, além da possibilidade
de ser conflitante com o processo de exploração interna, também
proposto neste trabalho.  No próximo capítulo, serão apresentadas as
conclusões deste trabalho, juntamente com possibilidades de
investigações futuras.

\section{Conclusão}\label{sec:conclusao}
Neste trabalho foram propostas duas estratégias de aplicação de
políticas parciais no Aprendizado por Reforço com Motivação
Intrínseca: aplicação de políticas parciais em pilha e exploração
interna à política parcial.  As estratégias propostas foram avaliadas
experimentalmente e comparadas a abordagens mais clássicas de
aprendizado e os resultados obtidos sugerem a aplicabilidade da
técnica para acelerar o processo de aprendizado através da aplicação
de políticas parciais com motivação intrínseca.  Percebeu-se ainda que
o parâmetro de exploração interna à política parcial possui impacto
significativo no desempenho de aprendizado, sendo o desempenho
apresentado diretamente proporcional a esse valor.

A estratégia proposta pode ser utilizada para o aprendizado automático
de políticas parciais em problemas de Aprendizado por Reforço,
contribuindo dessa forma para o campo de pesquisa em Transferência de
Conhecimento
\cite{beirigo2012,bergamo2011accelerating,bogdan2013forward,da2011geometric,da2011navigation,koga2013speeding,matos2011simultaneous,matos2011stochastic},
que busca acelerar o aprendizado do agente através do reuso de
características compartilhadas entre problemas distintos.  Como
trabalhos futuros, pretende-se esteder o algoritmo para a solução de
uma classe de problemas, em que os subproblemas comuns são
identificados e armazenados para uso futuro, dessa forma contribuindo
para acelerar a obtenção das soluções.

\begin{received}
\end{received}

% INCLUDE BIBLIOGRAPHY WHICH MUST FOLLOW kdmile.bst TEMPLATE
\bibliographystyle{kdmile}
\bibliography{kdmile}
% For information on how to write bibliography entries, 
% see file kdmileb.bib

\end{document}

#+ORGTBL: SEND thenameofthetable orgtbl-to-latex :splice nil :skip 0
|-------------+------------+----------+------|
| Objeto      | Ação       | Evento   | Mov. |
|             |            | saliente |      |
|-------------+------------+----------+------|
| alarme      | -          | \tmon\   | -    |
|             | -          | \tmoff\  | -    |
| botão A     | pressionar | \muon\   | -    |
|             | empurrar   | -        | V    |
| botão B     | pressionar | \muoff\  | -    |
|             | empurrar   | -        | V    |
| bola        | chutar     | -        | A    |
| interruptor | pressionar | \lion    | -    |
|             | pressionar | \lioff   | -    |
| sino        | -          | \beon\   | V    |
|-------------+------------+----------+------|
