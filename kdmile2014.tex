% by Mirella M. Moro; version: January/18/2012 @ 04:16pm
% -- 01/18/2012: more discussion on SBBD + kdmile; overall revision
% -- 09/03/2010: bib file with names for proceedings and journals; cls with shrinked {received}
% -- 08/27/2010: appendix, table example, more explanation within comments, editors' data

\documentclass[kdmile,a4paper]{kdmile} % NOTE: kdmile is published on A4 paper
\usepackage{graphicx,url}  % for using figures and url format
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}   % avoids warnings such as "LaTeX Font Warning: Font shape 'OMS/cmtt/m/n' undefined"
\usepackage[brazilian]{babel}
\usepackage[usenames,dvipsnames]{color}
\usepackage{soul,xcolor}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[normalem]{ulem}

%\usepackage{cite} % NOTE: do **not** include this package because it conflicts with kdmile.bst

% Standard definitions
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newdef{definition}[theorem]{Definition}
\newdef{remark}[theorem]{Remark}

% New environment definition
\newenvironment{latexcode}
{\ttfamily\vspace{0.1in}\setlength{\parindent}{18pt}}
{\vspace{0.1in}}

% Configurações gráficas
\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg} % Preferência por .eps (maior resolução)
\graphicspath{{fig/}}                           % Pasta padrão para as figuras

% Traduções ref. packaged 'algorithm[ic]'
\floatname{algorithm}{Algoritmo} % termo utilizado na referenciação
\renewcommand{\algorithmicrequire}{\textbf{Requer:}}
\renewcommand{\algorithmicensure}{\textbf{Assegurar-se:}}
\renewcommand{\algorithmicend}{\textbf{fim}}
\renewcommand{\algorithmicif}{\textbf{se}}
\renewcommand{\algorithmicthen}{\textbf{então}}
\renewcommand{\algorithmicelse}{\textbf{senão}}
\renewcommand{\algorithmicelsif}{\algorithmicelse\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicforall}{\textbf{para todo}}
\renewcommand{\algorithmicdo}{\textbf{faça}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{enquanto}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{loop}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{repita}}
\renewcommand{\algorithmicuntil}{\textbf{até}}
\renewcommand{\algorithmicprint}{\textbf{imprima}}
\renewcommand{\algorithmicreturn}{\textbf{retorne}}
\renewcommand{\algorithmictrue}{\textbf{verdadeiro}}
\renewcommand{\algorithmicfalse}{\textbf{falso}}
\renewcommand{\algorithmicand}{\textbf{e}}
\renewcommand{\algorithmicor}{\textbf{ou}}

% Eventos salientes
\newcommand{\tmon}{$alarme_{ON}$}
\newcommand{\tmoff}{$alarme_{OFF}$}
\newcommand{\muon}{$musica_{ON}$}
\newcommand{\muoff}{$musica_{OFF}$}
\newcommand{\lion}{$luz_{ON}$}
\newcommand{\lioff}{$luz_{OFF}$}
\newcommand{\beon}{$sino_{ON}$}

% Termos/conceitos
\newcommand{\imrl}{IMRL}
\newcommand{\mi}{motivação intrínseca}
\newcommand{\epS}{\xi_{p}}
\newcommand{\ep}{$\epS$}
\newcommand{\pilhaS}{\Pi}
\newcommand{\pilha}{$\pilhaS$}

\newcommand{\pps}{$\phi$}
\newcommand{\ppc}[1]{$\phi_{#1}$}


\newcommand{\attS}{a_{t+1}}
\newcommand{\att}{$\attS$}
\newcommand{\qbS}{Q^B}
\newcommand{\qb}{$\qbS$}
\newcommand{\td}{\emph{top-down}}
\newcommand{\bu}{\emph{bottom-up}}

% := e =:
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\eqdef}{=\vcentcolon}

% Comando para espaçamento vertical no ambiente algorithm
\newcommand{\GIMMESPACE}{\vspace{3.17 mm}}

% Para marcar palavras que precisam ser traduzidas
% \newcommand{\tr}[1]{\colorbox{MidnightBlue}{\textcolor{White}{\textbf{#1}}}}
\newcommand{\tr}[1]{\textcolor{MidnightBlue}{\textbf{#1}}}

% Para marcar partes do algoritmo original que foram modificadas
\sethlcolor{Green}
\newcommand{\grifar}[1]{\dotuline{#1}}
% \newcommand{\grifar}[1]{\hl{#1}}
% \newcommand{\grifar}[1]{\dashuline{#1}}
% \newcommand{\grifar}[1]{\uwave{#1}}

% Substituir pelo título quando estiver definido
\newcommand{\titulo}{Avaliação de Estratégias para Aplicação de\\ \ Políticas Parciais com Motivação Intrínseca}

%% Melhores comentários para o pacote algorithm
\renewcommand{\algorithmiccomment}[1]{#1}

% ALL FIELDS UNTIL BEGIN{document} ARE MANDATORY


% Includes headers with simplified name of the authors and article title
\markboth{R. L. Beirigo and V. Freire and A. H. R. Costa}
{\titulo}
%  -> \markboth{}{}
%         takes 2 arguments
%         ex: \markboth{A.Plastino}{Any article title}


% Title of the article
\title{\titulo}


% List of authors
%IF THERE ARE TWO or more institutions, please use:
%\author{Name of Author1\inst{1}, Name of Author2\inst{2}, Name of Author3\inst{2}}
\author{R. L. Beirigo and V. Freire and A. H. R. Costa}

%Affiliation and email
\institute{Escola Politécnica da Universidade de São Paulo, Brazil \\
  \email{\{rafaelbeirigo, valdinei.freire, anna.reali\}@usp.br}
}

% Article abstract - it should be from 100 to 300 words
\begin{abstract}
  As técnicas de Aprendizado por Reforço permitem a solução de um
  problema através da escolha de ações que maximizem valores de
  recompensas recebidas que refletem a qualidade das ações tomadas
  pelo agente em um processo de tentativa e erro. Em problemas com
  estrutura hierárquica, a solução final depende do encadeamento de
  soluções para sub-problemas aí presentes, sendo frequente a
  repetição de sub-problemas nesse encadeamento. Nesses casos, a
  utilização de políticas parciais permite o aprendizado e
  armazenamento das soluções individuais para cada sub-problema, que
  podem então ser utilizadas múltiplas vezes na composição de uma
   solução completa para o problema final, acelerando o
  aprendizado. Apesar de vantajosa, a utilização de políticas parciais
  necessita de definições por parte do projetista, o que representa
  uma sobrecarga. Para contornar esse problema, foram propostas
  técnicas de descoberta automática de políticas parciais, dentre as
  quais a utilização de motivação intrínseca se destaca por permitir
  ao agente aprender soluções de sub-problemas úteis na solução do
  problema final sem a necessidade de se definir manualmente novas
  recompensas para esses sub-problemas individualmente. Apesar de
  promissora, essa proposta utiliza um conjunto de componentes de
  aprendizado que ainda carece de investigação aprofundada acerca dos
  impactos individual e coletivo de cada componente, notadamente a
  aplicação das políticas parciais durante o aprendizado. Neste
  trabalho são investigadas estratégias de aplicação de políticas
  parciais durante o Aprendizado por Reforço com Motivação
  Intrínseca. Duas estratégias são propostas e o desempenho de cada
  estratégia é avaliado experimentalmente. Com base nesta avaliação,
  uma nova estratégia é proposta, a qual utiliza as alternativas que
  apresentaram o melhor desempenho na avaliação experimental.
\end{abstract}

% ACM Computing Classification System categories
\category{I.2.6}{Artificial Intelligence}{Learning} 

% Categories and Descriptors are available at the 1998 ACM Computing Classification System
% http://www.acm.org/about/class/1998/
%  -> \category{}{}{}
%         takes 3 arguments for the Computing Reviews Classification Scheme.
%         ex: \category{D.3.3}{Programming Languages}{Language Constructs and Features}
%                   [data types and structures]
%                   the last argument, in square brackets, is optional.

% Article keywords
\keywords{aprendizado por reforço, descoberta de políticas parciais, motivação intrínseca}
%  -> \keywords{} (in alphabetical order \keywords{document processing, sequences,
%                      string searching, subsequences, substrings})

% THE ARTICLE BEGINS
\begin{document}

% This is optional:
\begin{bottomstuff}
% TODO: preencher adequadamente
% similar to \thanks
% for authors' addresses; research/grant statements
\end{bottomstuff}

\maketitle

\section{Introdução}
Problemas nos quais o agente deve tomar uma série de decisões
sequenciais podem ser resolvidos através de técnicas de Aprendizado
por Reforço \cite{rlczaba}, em que o agente recebe um reforço numérico
indicando a qualidade de cada ação tomada. Esse processo de
aprendizado ocorre por tentativa e erro, no qual o agente alterna
entre a \emph{aplicação} do conhecimento adquirido até o momento para
selecionar a melhor ação e a \emph{exploração} do ambiente, onde o
agente experimenta novas ações, aumentando o seu conhecimento sobre o
ambiente e o problema. Apesar do sucesso apresentado na aplicação de
técnicas AR a problemas reais
\cite{tesauro1995tdgammon,barto1996elevator,scardua2002optimal,da2006inverse,abbeel2007helicopter},
elas podem apresentar indesejada lentidão na obtenção das soluções
\cite{barto2003hrl}.
 
Uma forma de mitigar essa lentidão é explorar características da
\emph{representação} do problema original que permitam a aplicação
de técnicas de otimização. Um exemplo disso é a utilização de
políticas parciais \cite{option1999}, que permitem que um problema
de AR originalmente com representação monolítica seja representado
de forma modular. Na nova representação, são determinados
sub-problemas componentes do problema original, permitindo o
aprendizado simultâneo da solução para o problema original e seus
sub-problemas. Quando o problema original possui uma estrutura
\emph{hierárquica}, é comum o agente ter de resolver mais de uma vez
o mesmo sub-problema em momentos diferentes. Nesses casos, a
utilização de políticas parciais apresenta a vantagem de acelerar o
aprendizado por eliminar a necessidade de se aprender a solução para
o mesmo sub-problema mais de uma vez, permitindo o reuso do
conhecimento já adquirido na forma de políticas parciais
\cite{option1999} e dessa forma acelerando o aprendizado.

Entretanto, para que as políticas parciais possam ser utilizadas, há
a necessidade da definição de elementos cuja identificação pode não
ser imediata à apreciação do problema, podendo causar sobrecarga em
tempo de projeto. Para contornar esse revés, são estudadas técnicas
de descoberta automática de políticas parciais
\cite{stolle2002learning,bakker2004hierarchical,mcgovern2001automatic,imrl2004},
em que a definição das mesmas fica a cargo de um algoritmo
responsável por identificar sub-problemas e criar políticas parciais
para resolvê-los. Das diversas técnicas empregadas, pode ser
destacada a utilização de \emph{motivação intrínseca} em AR
\cite{imrl2004}, em que o agente recebe recompensas adicionais
através de um mecanismo interno associado a variações de
características do ambiente que lhe despertam \emph{marcado
interesse}. Nessas técnicas, o agente é codificado de forma a
associar sub-problemas a variações significativas de atributos
selecionados na descrição fatorada do estado, reduzindo o trabalho
de definição das políticas parciais à determinação dos atributos
relevantes para a identificação dos sub-problemas.

Essa técnica foi aplicada com sucesso na solução de um problema com
forte estrutura hierárquica \cite{imrl2004}, através da proposta de um
algoritmo que utiliza em conjunto múltiplas técnicas de AR. Apesar da
aprensentação detalhada do papel da motivação intrínseca no
aprendizado, não há uma investigação mais aprofundada dos diversos
componentes de aprendizado envolvidos algoritmo, notadamente a
estratégia de aplicação das políticas parciais aprendidas. Neste
trabalho, investigamos essa questão, apresentando estratégias para a
aplicação das políticas parciais aprendidas juntamente com a avaliação
experimental de cada uma delas.  A primeira contribuição refere-se ao
armazenamento da aplicação de políticas parciais através de uma
estrutura de dados em forma de pilha, que armazena incrementalmente as
políticas parciais utilizadas pelo agente.  Para o processo de
terminação das políticas parciais e consequente remoção da pilha,
investiga-se o papel da exploração nesse processo.  Os resultados
experimentais obtidos sugerem a aplicabilidade das técnicas propostas
para o aprendizado de políticas parciais com a utilização de \mi.

\section{Fundamentos Teóricos}
Problemas de decisão sequencial são usualmente representados por um
\emph{Processo Markoviano de Decisão} (PMD) \cite{puterman2009markov},
definido pela quádrupa $<S, A, P, R>$, em que $S$ e $A$ são conjuntos
finitos de estados e ações respectivamente,
$R:S\rightarrow\Re$ representa a recompensa recebida ao
alcançar um estado $s \in S$ e $P:S\times A\times S \rightarrow [0,1]$
a probabilidade de alcançar um estado a partir de outro através da
execução de uma ação.  Uma política $\pi: S \rightarrow A$ associa uma
ação a ser executada em um estado e a função $V^{\pi}: S \rightarrow
\Re$ fornece o valor esperado de recompensas futuras ao,
partindo de $s \in S$, executar $\pi$.  A solução de um PDM é dado
pela política $\pi^* = \arg\max_{\pi}{V^{\pi}(s), \forall s \in S}$,
\emph{exploração}/\emph{aplicação}.  denominada \emph{ótima}, que pode
ser obtida através do Q-Learning \cite{qlearning}, uma técnica de
Aprendizado por Reforço (AR). Nesse algoritmo, o a seleção da ações a
serem executadas é feita pela estratégia $\epsilon$-gulosa, $\epsilon
\in [0,1]$, que retorna ações aleatórias com probabilidade $\epsilon$
e gulosas com probabilidade $1 - \epsilon$, configurando o compromisso

Por ser realizado através de tentativa e erro esse processo é
geralmente lento e pode ser acelerado pela utilização de políticas
parciais \cite{option1999} \pps$\ \defeq\ <\mathcal{I},\ \pi, \beta>$,
em que $\mathcal{I} \subseteq S$ indica os estados em que a política
parcial \pps\ pode ser iniciada, $\beta: S \rightarrow [0,1]$ a
probabilidade de terminar \pps\ em um estado $s \in S$ e $\pi:S
\rightarrow \Phi$, associa ações primitivas ou políticas parciais a
serem executas, onde $\Phi$ corresponde ao conjunto de todas as ações
primitivas e políticas parciais.  A aplicação de políticas parciais
conjuntamente às ações primitivas possui forte caráter
\emph{hierárquico}, dado que a obtenção da solução do problema
original é realizada através da divisão deste em sub-problemas
menores, cuja solução é obtida através do aprendizado de políticas
parciais.

\setlength\intextsep{0pt}
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{center}
      \begin{algorithm}[H]
        \caption{\imrl}
        \label{alg:imrl}
        \algsetup{indent=1em}
        \small
        \begin{algorithmic}[1]
          \newcommand{\sd}{s_{t+1}}
          \newcommand{\bo}[1]{$\beta^o(#1)$}
          \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
          \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
          \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
          \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
          \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
          \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
          \newcommand{\termoA}{\bo{\sd}$v$}
          \newcommand{\termoB}{[\ubo{\sd}][\maos]}
          \newcommand{\termoC}{\bo{x}$v$}
          \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
          \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

          \REQUIRE{Inicializar $s_t,\ a_t,\ $$o_t$$,\ r^e_t,\ r^i_t$}
          \REPEAT
          \STATE\COMMENT{Obter $\sd$, $r^e_{t+1}$ por $s_t,\ a_t \rightarrow s_{t+1}$}
          \STATE\COMMENT{Tratar Ocorrência de Evento Saliente}
          \STATE\COMMENT{Atualizar Modelo (Políticas Parciais)}
          \STATE\COMMENT{Atualizar Função-Valor (Q-Learning)}
          \STATE\COMMENT{Atualizar Módulo de Planejamento}
          \STATE\COMMENT{Atualizar Funções-Valor (Políticas Parciais)}
          \STATE\COMMENT{Obter $a_{t+1}$ (estratégia $\epsilon$-gulosa)}
          \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}
          \UNTIL{condição de parada}
        \end{algorithmic}
      \end{algorithm}
    \end{center}
  \end{minipage}
\end{wrapfigure}
A obtenção de políticas parciais
 pode ser realizada de forma
automática simultaneamente
à solução do problema original através da
utilização de motivação intrínseca para identificação de sub-tarefas e
correspondente aprendizado de políticas parciais.  O algoritmo
proposto em \cite{imrl2004} (IMRL), apresentado ao lado, implementa o
conceito de \emph{recompensas intrínsecas}, que são recebidas quando
ocorrem variações significativas de determinados atributos presentes
da descrição fatorada do estado, sendo denominados \emph{atributos
    salientes} e a ocorrência da variação denominada \emph{evento
    saliente}, sendo o conceito utilizado para a obtenção, atualização
e aplicação do modelo e função valor para a obtenção das políticas
parciais.

\section{Proposta}
Políticas parciais podem ser obtidas automaticamente simultaneamente
ao aprendizado através do algoritmo apresentado em \cite{imrl2004},
que utiliza motivação intrínseca para detectar a presença de
sub-problemas e associar a cada um deles uma política parcial para
resolvê-lo.  Entretanto, o algoritmo proposto concentra-se nos
aspectos relacionados à \emph{obteção} da políticas parciais.  Neste
trabalho, são investigados dois aspectos da \emph{aplicação} das
políticas parciais: exploração interna e método de terminação, através
da adição de duas funcionalidades ao algoritmo original.

No momento da obtenção da próxima ação, feita de forma aleatória ou
gulosa, caso a próxima ação selecionada seja uma política parcial
\ppc{1}, esta é adicionada ao topo da pilha \pilha.  Dessa forma, um
agente que inicia com uma pilha vazia $\pilhaS=[\ ]$ e obtém como
próxima ação a política parcial \ppc{1} resultará com
$\pilhaS=[$\ppc{1}$]$.  Caso a aplicação de \ppc{1} sugira a execução
de uma nova política parcial \ppc{2}, essa será então empilhada,
resultando em $\pilhaS=[$\ppc{1}$,\ $\ppc{2}$]$ e assim sucessivamente
para novas políticas parciais.  Uma política parcial é removida da
pilha no momento em que seu objetivo é atingido pelo agente, ou seja,
o evento saliente utilizado para criá-la é disparado.  Como uma
política parcial \pps\ pode invocar outras de modo a atingir seu
objetivo, \pps\ é removida da pilha juntamente com as políticas
parciais invocadas por ela.  A estratégia de remoção testa as
políticas parciais para remoção partindo da base da pilha em direção
ao topo, ou seja, prioriza a remoção das políticas parciais mais
anteriormente adicinadas.

Propõe-se também um mecanismo de \emph{exploração interna} à política
parcial, elaborada para atenuar o efeito de \emph{rigidez}
potencialmente imposto pela política parcial, em que o agente fica
preso a uma sequência de ações. Dessa forma, durante a aplicação de
uma política parcial por parte do agente, no momento da escolha da
próxima ação a ser executada, originalmente essa escolha seria feita
sempre de forma gulosa somente.  Isso foi feito porque, apesar de a
estratégia gulosa ser a melhor no caso de a política parcial ser ótima
para o sub-problema sendo resolvido, ela implica em uma restrição que
potencialmente atrapalharia o desempenho de aprendizado do agente nos
passos iniciais, em que as políticas parciais estão em fase inicial de
aprendizado.  Dessa forma, o parâmetro $\epS \in [0,1]$ indica o
percentual de exploração utilizado na aplicação da política parcial.
No momento da aplicação da política parcial, é sorteado um valor no
intervalo $[0,1]$; caso esse valor seja maior que \ep\, o agente
aplica a política parcial sendo seguida de forma gulosa e, em caso
contrário, o agente realiza exploração, ou seja, executa uma ação
aleatória dentre as possíveis para o estado atual.

% O Algoritmo \ref{alg:armipep} representa a proposta em pseudo-código.

% \begin{algorithm}
% \caption{ARMIP\ep}
% \label{alg:armipep}
% \algsetup{indent=2em}
% \small
% \begin{algorithmic}[1]
%     \newcommand{\sd}{s_{t+1}}
%     \newcommand{\bo}[1]{$\beta^o(#1)$}
%     \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
%     \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
%     \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
%     \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
%     \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
%     \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
%     \newcommand{\termoA}{\bo{\sd}$v$}
%     \newcommand{\termoB}{[\ubo{\sd}][\maos]}
%     \newcommand{\termoC}{\bo{x}$v$}
%     \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
%     \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

%     \REQUIRE{Inicializar $s_t,\ a_t,\ $\grifar{$o_t$}$,\ r^e_t,\ r^i_t$}

%     \REPEAT

%     \STATE Obter próximo estado $\sd$ e recompensa extrínsica $r^e_{t+1}$ recebida na transição $s_t,\ a_t \rightarrow s_{t+1}$}

%     \STATE\COMMENT{Tratar ocorrência de evento saliente}

%     \STATE\COMMENT{Atualizar modelo das políticas parciais}

%     \STATE\COMMENT{Atualizar função valor estado-ação do Q-Learning, $Q_B$}

%     \STATE\COMMENT{Atualizar funções valor estado-ação das políticas parciais}

%     \STATE Escolher $a_{t+1}$ através de estratégia $\epsilon$-gulosa
%     com respeito a $Q_B$; aplicar políticas parciais por estratégia
%     \ep-gulosa; empilhar ocorrências de políticas parciais 

%     \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}

%     \UNTIL{condição de parada}
% \end{algorithmic}
% \end{algorithm}

% No Algoritmo \ref{alg:modificado} encontram-se marcadas as
% modificações propostas em relação ao algoritmo original
% \cite{imrl2004}. O intuito é o de contribuir à proposta original
% soluções de eventuais ambiguidades ou aparentes inconsistências,
% dessa forma apresentando um algoritmo que, mantendo o espírito
% original da utilização de Motivação Instrínseca no Aprendizado por
% Reforço, acrescenta alternativas de implementação buscando obter
% melhorias de desempenho no aprendizado. Abaixo são descritas as
% motivações e respectivas motivações teóricas e/ou de cunho prático.

% Nas linhas \ref{alg:mod:ind:model}, \ref{alg:mod:ind:ql} e
% \ref{alg:mod:ind:qo}, os índices de $r^e_t$ e $r^i_t$ foram
% modificados para $r^e_{t+1}$ e $r^i_{t+1}$, respectivamente. Isso
% foi feito para permitir que as atualizações que utilizam esses
% valores considerassem a transição $s_t,\ a_t \leftarrow s_{t+1}$ ao
% invés da precedente a esta, como era anteriormente. Ainda de acordo
% com essa modificação, a obtenção de $r^e_{t+1}$, que anteriormente
% era realizada após a linha \ref{alg:mod:re2:localantigo}, passa a
% ser realizada logo após a transição de estados (linha
% \ref{alg:mod:re2}).

% A atualização do modelo da política parcial na proposta original, é
% precedida de um teste para evitar que a política parcial
% correspondente ao evento saliente que possa ter ocorrido no passo
% atual, $o_e$, seja processada (linha \ref{alg:mod:oe:if}). Uma
% vantagem advinda disso é o de se poder utilizar os valores de
% $Q^{o_e}$ já atualizados. Entretanto, como a atualização de
% $Q^{o_e}$ somente ocorre ao final do algoritmo, é necessário esperar
% os próximos passos para utilizar os valores atualizados. Dessa
% forma, e como a primeira referência a $o_e$ ocorre na linha
% \ref{alg:mod:oe:priocorr}, na nova proposta a inicialização e
% atribuição foram adicionadas nas linhas \ref{alg:mod:oe:ini} e
% \ref{alg:mod:oe:atr}, respectivamente. Isso foi feito por dois
% motivos: (i) definir $o_e$ de forma explícita no algoritmo e (ii)
% tratar os valores que $o_e$ assume de acordo com a lógica de
% atualização do modelo da política parcial, aproveitando os valores
% atualizados de $Q^{o_e}$.

% O algoritmo original prevê a adição de estados às políticas parciais
% de forma retroativa: caso o agente, partindo de $s_t$ alcance
% $s_{t+1}$ e $s_{t+1} \in I^o$, $s_t$ é adicionado a
% $I^o$. Entretanto, caso uma política parcial possa ser reiniciada em
% um estado terminal, pode ser gerado um ciclo de repetição indefinido
% em que a política parcial chama a si mesma ao terminar. Dessa forma,
% na linha \ref{alg:mod:addI} foi adicionado um teste para evitar que
% um estado terminal da política parcial seja adicionado ao seu
% conjunto iniciador.

% Como a escolha de quais ações são aplicáveis nesse momento é um
% fator importante na atualização do modelo das políticas parciais, na
% proposta original, as funções $P^o$ e $R^o$ somente são atualizadas
% caso a ação $a_t$ executada no passo atual seja gulosa para a
% política parcial em questão (linha \ref{alg:mod:atgreedy}). Isso
% pode ser explicado pelo fato de que a atualização dessas funções
% representem um consumo significativo de recursos computacionais. Na
% proposta atual, essa restrição foi investigada, sendo executados
% experimentos também para o caso em que não há a necessidade de $a_t$
% ser a ação gulosa.

% Na proposta original, a atualização de $Q_B$ para políticas parciais
% (planejamento SMDP, linha \ref{alg:mod:qbo}) considerava para esse
% cálculo quaiquer estados $s_t$ que o agente viesse a visitar. Para
% evitar problemas de incosistência entre a função-valor e o conjunto
% iniciador de uma política parcial $o$, foi adicionado um teste que
% somente atualiza $Q_B(s_t,\ o)$ para uma dada política parcial $o$
% se $s_t \in I^o$. Dessa forma, evita-se que seja atribuído um valor
% positivo à execução de uma política parcial $o$ em um estado em que
% a política parcial não possa ser executada. Uma outra vantagem
% decorrente dessa modificação é a possível economia de recursos
% computacionais, já que algumas atualizações desnecessárias são
% potencialmente evitadas.

% \begin{algorithm}
% \caption{Algoritmo X}
% \label{alg:modificado}
% \algsetup{indent=2em}
% \small
% \begin{algorithmic}[1]
%     \newcommand{\sd}{s_{t+1}}
%     \newcommand{\bo}[1]{$\beta^o(#1)$}
%     \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
%     \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
%     \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
%     \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
%     \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
%     \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
%     \newcommand{\termoA}{\bo{\sd}$v$}
%     \newcommand{\termoB}{[\ubo{\sd}][\maos]}
%     \newcommand{\termoC}{\bo{x}$v$}
%     \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
%     \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

%     \REQUIRE{Inicializar $s_t,\ a_t,\ $\grifar{$o_t$}$,\ r^e_t,\ r^i_t$}

%     \REPEAT

%     \STATE Obter próximo estado $\sd$

%     \STATE \label{alg:mod:re2} \grifar{$r^e_{t+1} \leftarrow$ recompensa extrínsica recebida na transição $s_t,\ a_t \rightarrow s_{t+1}$}

%     \GIMMESPACE

%     \STATE\COMMENT{Trata ocorrência de evento saliente}

%     \STATE \label{alg:mod:oe:ini}\grifar{$o_e \leftarrow nil$}

%     \IF{ocorreu um evento saliente $e$}

%         \IF{política parcial para $e$, $o_e$, não existe em $O$}\label{alg:mod:oe:priocorr}

%             \STATE \label{alg:mod:oe:atr}\grifar{$o_e \leftarrow $[política parcial para $e$]}

%             \STATE Criar $o_e$ em $O$

%             \STATE Adicionar $s_t$ a $I^{o_e}$

%             \STATE $\beta^{o_e}(s_{t+1}) \leftarrow 1$

%         \ENDIF

%         \STATE $r^i_{t+1} \leftarrow \tau[1-P(s_{t+1}|s_t)]$ \COMMENT{$\tau$ constante}

%     \ELSE

%         \STATE $r^i_{t+1} \leftarrow 0$

%     \ENDIF

%     \GIMMESPACE

%     \STATE\COMMENT{Atualiza o modelo das políticas parciais}

%     \FOR{cada política parcial $o \neq $\grifar{$o_e$} em $O$} \label{alg:mod:oe:if}

%         \IF{$s_{t+1} \in I^o$ \AND \grifar{$\beta^o(s_t) \neq 1$}} \label{alg:mod:addI}

%             \STATE Adicionar $s_t$ a $I^o$

%         \ENDIF

%         \IF{\grifar{$a_t$ é a ação gulosa para $o$ no estado $s_t$}} \label{alg:mod:atgulosa}

%             \STATE\COMMENT{Atualiza o modelo probabilístico de transição da política parcial}

%             \STATE $P(x|s_t) \xleftarrow{\alpha} [\gamma(1 - \beta^o(s_{t+1}))P^o(x|s_{t+1}) + \gamma\beta^o(s_{t+1})\delta_{s_{t+1}x}]$

%             \STATE\COMMENT{Atualiza modelo de recompensa da política parcial} \STATE $R^o(s_t) \xleftarrow{\alpha} [$\grifar{$r^e_{t+1}$}$ + \gamma(1 - \beta^o(s_{t+1}))R^o(s_{t+1})]$ \label{alg:mod:ind:model}

%         \ENDIF

%     \ENDFOR

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização da \tr{função valor estado-ação} do Q-Learning}

%     \STATE $Q_B(s_t,\ a_t) \xleftarrow{\alpha} $\grifar{$r^e_{t+1}$}$ + $\grifar{$r^i_{t+1}$}$ + \gamma\max_{a \in A \cup O}{Q_B(s_{t+1},\ a)}$ \label{alg:mod:ind:ql}

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização da \tr{função valor estado-ação} para planejamento SMDP}

%     \FOR{cada política parcial $o$ em $O$}

%         \IF{\grifar{$s_t \in I^o$}}

%         \STATE $Q_B(s_t,\ o) \xleftarrow{\alpha} R^o(s_t) + \sum_{x \in S}{P^o(x|s_t)\max_{a \in A \cup O}Q_B(x,\ a)}$ \label{alg:mod:qbo}

%         \ENDIF

%     \ENDFOR

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização das \tr{funções valor estado-ação} para as políticas parciais}

%     \FOR{cada política parcial $o$ em $O$ tal que $s_t \in I^o$}

%         \STATE $v \leftarrow$ \tv

%         \STATE $Q^o(s_t,\ a_t)$\aalpha$\ $\grifar{$r^e_{t+1}$}$\ +\ \gamma\{$\termoA$\ +\ $\termoB$\}$ \label{alg:mod:ind:qo}

%         \FOR{cada política parcial $o'$ em $O$ tal que $s_t \in I^{o'}$ e $o \neq o'$}

%             \STATE $Q^o(s_t,\ o')$\aalpha $R^{o'}(s_t)\ + \sum_{x \in S}{P^{o'}(x|s_t)}\{$\termoC$\ +\ $\termoD$\}$

%         \ENDFOR

%     \ENDFOR \label{alg:mod:re2:localantigo}

%     \STATE Escolher $a_{t+1}$ através de estratégia \tr{$\epsilon$-gulosa} com respeito a $Q_B$

%     \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}

%     \UNTIL{condição de parada}

% \end{algorithmic}
% \end{algorithm}

\section{Avaliação Experimental}
O domínio utilizado na avaliação experimental foi o \emph{Playroom}
\cite{imrl2004}, em que o agente se econtra em um reticulado
$5\times5$ e deve descobrir como ativar um \emph{alarme} interagindo
com objetos aí
presentes. O agente é composto de um \emph{olho}, uma \emph{mão} e um
\emph{alvo} e possui sempre disponíveis as seguintes ações: apontar
\emph{olho} para a célula onde se encontra a \emph{mão} ou para a
célula onde se encontra o \emph{alvo}; mover a \emph{mão} ou o
\emph{alvo} para a célula atualmente apontada pelo \emph{olho}.

\begin{wrapfigure}[14]{r}{0.45\textwidth}
  \newcommand{\fppt}{\fontsize{.295cm}{1em}\selectfont}
  \fppt
  \begin{center}
    % BEGIN RECEIVE ORGTBL thenameofthetable
    \begin{tabular}{llll}
      \hline
      Objeto & Ação & Evento & Mov. \\
      &  & saliente &  \\
      \hline
      alarme & - & \tmon\ & - \\
      & - & \tmoff\ & - \\
      botão A & pressionar & \muon\ & - \\
      & empurrar & - & V \\
      botão B & pressionar & \muoff\ & - \\
      & empurrar & - & V \\
      bola & chutar & - & A \\
      interruptor & pressionar & \lion & - \\
      & pressionar & \lioff & - \\
      sino & - & \beon\ & V \\
      \hline
    \end{tabular}
    % END RECEIVE ORGTBL thenameofthetable
  \end{center}
  \caption{\fppt \emph{Playroom}: objetos, ações e eventos salientes}
  \label{fig:obj}
\end{wrapfigure}

Na tabela ao lado estão relacionados os objetos aí presentes,
juntamente com as ações e o evento
saliente que disparam.  Essas ações
ficam disponíveis somente quando o \emph{olho} e a \emph{mão} estão
sobre a célula em que se encontra o objeto.  Na coluna \emph{Mov.},
objetos marcados com \emph{A} movem-se para a posição atual do
\emph{alvo} e as marcadas com \emph{V} para uma célula vizinha
aleatória ao serem ativados.  Caso a \emph{bola} seja chutada com o
\emph{alvo} sobre o \emph{sino}, este é ativado, disparando o evento
\beon.  O \emph{alarme} dispara quando, com a música ligada e a luz
desligada, ocorre \beon\ e é desativado quando o estado da luz ou da
música é modificado; o agente somente consegue diferenciar os botões e
executar as ações associadas a eles se a luz estiver ligada.

Para avaliar a técnica proposta, foram realizados dois experimentos,
sendo que em um deles o módulo de obtenção de políticas parciais a
partir de \mi\ foi desativado, ou seja, o agente somente receberia
recompensas extrínsecas, sem a obtenção de políticas parciais.  Foram
realizados 100 experimentos, de 500 mil passos cada, com o agente
recebendo uma recompensa extrínseca para ocorrência de \tmon\ no valor
de 10 e recompensas intrínsecas para os demais eventos salientes no
valor de 1.  Os resultados são apresentados no gráfico da Figura
\ref{fig:mi}, que apresenta a média de passos necessários para ativar
\tmon\ por quantidade de ativação. Os resultados aí presentes sugerem
maior eficiência de aprendizado por parte do agente que utiliza \mi\,
pois este diminui o tempo necessário para a solução do problema com
uma maior rapidez em relação ao agente que utiliza somente motivação
extrínseca.

No gráfico da Figura \ref{fig:ri} são apresentados os valores de
recompensa intrínseca recebida por passo do experimento, evidenciando
algumas características do aprendizado utilizando \mi : o agente
inicialmente procede à execução das tarefas mais simples, como
ligar/desligar a luz, o que pode ser visto pela frequência e
prematuridade com que o agente recebe recompensas intrínsecas para
essas tarefas. Pode-se perceber também uma variação dos valores
recebidos associados ao evento saliente com a repetição dos mesmos: há
uma tendência de \emph{diminuição} dos valores recebidos para eventos
salientes repetidos, mostrando a capacidade da técnica de priorizar a
realização de tarefas menos conhecidas.

Para avaliar o impacto da utilização de exploração interna à política
parcial, foram realizados 100 experimentos de 500 mil passos cada com
a mesma parametrização de recompensas anterior e com valores de $\epS
\in \{0.2, 0.4, 0.6, 0.8\}$.  O desempenho foi comparado com um agente
utilizando Q-Learning e outro com comportamento aleatório e os
resultados estão presentes na Figura \ref{fig:ep}, que apresenta a
média cumulativa de ativações do \tmon\ por passo de experimento.  A
análise do gráfico sugere que a utilização da técnica proposta para
obtenção e aplicação de políticas parciais pode superar a utilização
de algoritmos que não utilizam políticas parciais.  Pode-se também
perceber que o valor de \ep\ possui impacto significativo no
aprendizado, sendo o desempenho apresentado diretamente proporcional
ao valor de \ep.

\begin{figure}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
      \includegraphics[width=\textwidth]{EfeitoMI}
      \caption{Efeito da Motivação Intrínseca no Aprendizado}
      \label{fig:mi}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
      \includegraphics[width=\textwidth]{toyonxstep-mediaacum-epsopt-parcial-02-04-06-08}
      \caption{Influência de \ep\ no Aprendizado}
      \label{fig:ep}
    \end{subfigure}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{r_i-2014-07-12_07-15-03-Teri}
    \caption{Recebimento de Recompensa Intrínseca Durante o Aprendizado}
    \label{fig:ri}
  \end{subfigure}
  \caption{}\label{fig:res}
\end{figure}

\section{Conclusão}
Neste trabalho foram propostas duas estratégias de aplicação de
políticas parciais no Aprendizado por Reforço com Motivação
Intrínseca: aplicação de políticas parciais em pilha e exploração
interna à política parcial.  Experimentos foram realizados de forma a
validar a proposta, nos quais o agente armazenava as políticas
parciais aplicadas durante o o aprendizado, removendo-as através de
estratégia proposta.  A aplicação das políticas parciais foi
modificada de forma a permitir exploração interna, com a qual o agente
poderia contornar uma eventual restritividade por parte da aplicação
das políticas parciais.  Realizou-se uma comparação da técnica de
aprendizado proposta frente a abordagens mais clássicas de aprendizado
e comportamento aleatório.  Os resultados obtidos sugerem a
aplicabilidade da técnica para acelerar o processo de aprendizado.
Percebeu-se ainda que o parâmetro de exploração interna à política
parcial possui impacto significativo no desempenho de aprendizado,
sendo o desempenho apresentado diretamente proporcional a esse valor.

A estratégia proposta pode ser utilizada para o aprendizado automático
de políticas parciais em problemas de Aprendizado por Reforço,
contribuindo dessa forma para o campo de pesquisa em Transferência de
Conhecimento
\cite{beirigo2012,bergamo2011accelerating,bogdan2013forward,da2011geometric,da2011navigation,koga2013speeding,matos2011simultaneous,matos2011stochastic},
que busca acelerar o aprendizado do agente através do reuso de
características compartilhadas entre problemas distintos.  Como
trabalhos futuros, pretende-se esteder o algoritmo para a solução de
uma classe de problemas, em que os sub-problemas comuns são
identificados e armazenados para uso futuro, dessa forma contribuindo
para acelerar a obtenção das soluções.

\begin{received}
\end{received}

% INCLUDE BIBLIOGRAPHY WHICH MUST FOLLOW kdmile.bst TEMPLATE
\bibliographystyle{kdmile}
\bibliography{kdmile}
% For information on how to write bibliography entries, 
% see file kdmileb.bib

\end{document}

#+ORGTBL: SEND thenameofthetable orgtbl-to-latex :splice nil :skip 0
|-------------+------------+----------+------|
| Objeto      | Ação       | Evento   | Mov. |
|             |            | saliente |      |
|-------------+------------+----------+------|
| alarme      | -          | \tmon\   | -    |
|             | -          | \tmoff\  | -    |
| botão A     | pressionar | \muon\   | -    |
|             | empurrar   | -        | V    |
| botão B     | pressionar | \muoff\  | -    |
|             | empurrar   | -        | V    |
| bola        | chutar     | -        | A    |
| interruptor | pressionar | \lion    | -    |
|             | pressionar | \lioff   | -    |
| sino        | -          | \beon\   | V    |
|-------------+------------+----------+------|
