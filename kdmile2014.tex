% by Mirella M. Moro; version: January/18/2012 @ 04:16pm
% -- 01/18/2012: more discussion on SBBD + kdmile; overall revision
% -- 09/03/2010: bib file with names for proceedings and journals; cls with shrinked {received}
% -- 08/27/2010: appendix, table example, more explanation within comments, editors' data

\documentclass[kdmile,a4paper]{kdmile} % NOTE: kdmile is published on A4 paper
\usepackage{graphicx,url}  % for using figures and url format
\usepackage[T1]{fontenc}   % avoids warnings such as "LaTeX Font Warning: Font shape 'OMS/cmtt/m/n' undefined"
\usepackage[brazilian]{babel}
\usepackage[usenames,dvipsnames]{color}
\usepackage{soul,xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{ulem}

%\usepackage{cite} % NOTE: do **not** include this package because it conflicts with kdmile.bst

% Standard definitions
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newdef{definition}[theorem]{Definition}
\newdef{remark}[theorem]{Remark}

% New environment definition
\newenvironment{latexcode}
{\ttfamily\vspace{0.1in}\setlength{\parindent}{18pt}}
{\vspace{0.1in}}

% Configurações gráficas
\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg} % Preferência por .eps (maior resolução)
\graphicspath{{fig/}}                           % Pasta padrão para as figuras

% Traduções ref. packaged 'algorithm[ic]'
\floatname{algorithm}{Algoritmo} % termo utilizado na referenciação
\renewcommand{\algorithmicrequire}{\textbf{Requer:}}
\renewcommand{\algorithmicensure}{\textbf{Assegurar-se:}}
\renewcommand{\algorithmicend}{\textbf{fim}}
\renewcommand{\algorithmicif}{\textbf{se}}
\renewcommand{\algorithmicthen}{\textbf{então}}
\renewcommand{\algorithmicelse}{\textbf{senão}}
\renewcommand{\algorithmicelsif}{\algorithmicelse\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicforall}{\textbf{para todo}}
\renewcommand{\algorithmicdo}{\textbf{faça}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{enquanto}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{loop}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{repita}}
\renewcommand{\algorithmicuntil}{\textbf{até}}
\renewcommand{\algorithmicprint}{\textbf{imprima}}
\renewcommand{\algorithmicreturn}{\textbf{retorne}}
\renewcommand{\algorithmictrue}{\textbf{verdadeiro}}
\renewcommand{\algorithmicfalse}{\textbf{falso}}
\renewcommand{\algorithmicand}{\textbf{e}}
\renewcommand{\algorithmicor}{\textbf{ou}}

% Comando para espaçamento vertical no ambiente algorithm
\newcommand{\GIMMESPACE}{\vspace{3.17 mm}}

% Para marcar palavras que precisam ser traduzidas
% \newcommand{\tr}[1]{\colorbox{MidnightBlue}{\textcolor{White}{\textbf{#1}}}}
\newcommand{\tr}[1]{\textcolor{MidnightBlue}{\textbf{#1}}}

% Para marcar partes do algoritmo original que foram modificadas
\sethlcolor{Green}
\newcommand{\grifar}[1]{\dotuline{#1}}
% \newcommand{\grifar}[1]{\hl{#1}}
% \newcommand{\grifar}[1]{\dashuline{#1}}
% \newcommand{\grifar}[1]{\uwave{#1}}

% Substituir pelo título quando estiver definido
\newcommand{\titulo}{Estratégias de Aplicação de Options no Aprendizado por Reforço com Motivação Intrínseca}

%% Melhores comentários para o pacote algorithm
\renewcommand{\algorithmiccomment}[1]{// #1}

% ALL FIELDS UNTIL BEGIN{document} ARE MANDATORY


% Includes headers with simplified name of the authors and article title
\markboth{R. L. Beirigo and V. F. da Silva and A. H. R. Costa}
{\titulo}
%  -> \markboth{}{}
%         takes 2 arguments
%         ex: \markboth{A.Plastino}{Any article title}


% Title of the article
\title{\titulo}


% List of authors
%IF THERE ARE TWO or more institutions, please use:
%\author{Name of Author1\inst{1}, Name of Author2\inst{2}, Name of Author3\inst{2}}
\author{R. L. Beirigo and V. F. da Silva and A. H. R. Costa}

%Affiliation and email
\institute{Escola Politécnica da Universidade de São Paulo, Brazil \\
  \email{\{rafaelbeirigo, valdinei.freire, anna.reali\}@usp.br}
}

% Article abstract - it should be from 100 to 300 words
\begin{abstract}
  As técnicas de Aprendizado por Reforço permitem a solução de um problema através da escolha de ações que maximizem valores de recompensas recebidas e que refletem a qualidade das ações tomadas pelo agente. Em problemas com estrutura hierárquica, a solução final depende do encadeamento de soluções para sub-problemas aí presentes, sendo frequente a repetição de sub-problemas nesse encadeamento. Nesses casos, a utilização de options permite o aprendizado e armazenamento das soluções individuais para cada sub-problema, que podem então ser utilizadas múltiplas vezes na composição de uma solução completa para o problema final. Apesar de vantajosa, a utilização de options necessita de definições que podem representar uma significativa sobrecarga para o projetista. Para contornar esse problema, foram propostas técnicas de descoberta automática de options, dentre as quais a utilização de motivação intrínseca apresenta resultados promissores. A utilização da motivação intrínseca para a descoberta de options permite ao agente aprender soluções de sub-problemas úteis na solução do problema final sem a sobrecarga de se definir novas recompensas para esse aprendizado de forma manual. Apesar de promissora, a proposta de descoberta automática de options por motivação intrínseca utiliza uma variedade de componentes de aprendizado que ainda carece de investigação aprofundada acerca dos impactos individual e coletivo de cada um, notadamente a aplicação das options durante o aprendizado. Este trabalho apresenta uma investigação de estratégias de aplicação de options durante o Aprendizado por Reforço com Motivação Intrínseca através da implementação das estratégias propostas, acompanhadas de avaliação de desempenho por verificação experimental e breve discussão teórica. Os resultados obtidos neste trabalho são compilados em uma proposta de implementação que utiliza as alternativas que apresentaram o melhor desempenho de aprendizado.
\end{abstract}

% ACM Computing Classification System categories
\category{I.2.6}{Artificial Intelligence}{Learning} 

% Categories and Descriptors are available at the 1998 ACM Computing Classification System
% http://www.acm.org/about/class/1998/
%  -> \category{}{}{}
%         takes 3 arguments for the Computing Reviews Classification Scheme.
%         ex: \category{D.3.3}{Programming Languages}{Language Constructs and Features}
%                   [data types and structures]
%                   the last argument, in square brackets, is optional.

% Article keywords
\keywords{aprendizado por reforço, descoberta de options, motivação intrínseca}
%  -> \keywords{} (in alphabetical order \keywords{document processing, sequences,
%                      string searching, subsequences, substrings})

% THE ARTICLE BEGINS
\begin{document}

% This is optional:
\begin{bottomstuff}
% TODO: preencher adequadamente
% similar to \thanks
% for authors' addresses; research/grant statements
\end{bottomstuff}

\maketitle

\section{Introdução}
Pellentesque dapibus suscipit ligula. Donec posuere augue in quam. Etiam vel tortor sodales tellus ultricies commodo. Suspendisse potenti. Aenean in sem ac leo mollis blandit. Donec neque quam, dignissim in, mollis nec, sagittis eu, wisi. Phasellus lacus. Etiam laoreet quam sed arcu. Phasellus at dui in ligula mollis ultricies. Integer placerat tristique nisl. Praesent augue. Fusce commodo. Vestibulum convallis, lorem a tempus semper, dui dui euismod elit, vitae placerat urna tortor vitae lacus. Nullam libero mauris, consequat quis, varius et, dictum id, arcu. Mauris mollis tincidunt felis. Aliquam feugiat tellus ut neque. Nulla facilisis, risus a rhoncus fermentum, tellus tellus lacinia purus, et dictum nunc justo sit amet elit.

\section{Fundamentos Teóricos}
Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Donec hendrerit tempor tellus. Donec pretium posuere tellus. Proin quam nisl, tincidunt et, mattis eget, convallis nec, purus. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Nulla posuere. Donec vitae dolor. Nullam tristique diam non turpis. Cras placerat accumsan nulla. Nullam rutrum. Nam vestibulum accumsan nisl.

\section{Algoritmo Proposto}\label{sec:algoritmo}
No Algoritmo \ref{alg:modificado} encontram-se marcadas as modificações propostas em relação ao algoritmo original \cite{imrl}. O intuito é o de contribuir à proposta original soluções de eventuais ambiguidades ou aparentes inconsistências, dessa forma apresentando um algoritmo que, mantendo o espírito original da utilização de Motivação Instrínseca no Aprendizado por Reforço, acrescenta alternativas de implementação buscando obter melhorias de desempenho no aprendizado. Abaixo são descritas as motivações e respectivas motivações teóricas e/ou de cunho prático.

Nas linhas \ref{alg:mod:ind:model}, \ref{alg:mod:ind:ql} e \ref{alg:mod:ind:qo}, os índices de $r^e_t$ e $r^i_t$ foram modificados para $r^e_{t+1}$ e $r^i_{t+1}$, respectivamente. Isso foi feito para permitir que as atualizações que utilizam esses valores considerassem a transição $s_t,\ a_t \leftarrow s_{t+1}$ ao invés da precedente a esta, como era anteriormente. Ainda de acordo com essa modificação, a obtenção de $r^e_{t+1}$, que anteriormente era realizada após a linha \ref{alg:mod:re2:localantigo}, passa a ser realizada logo após a transição de estados (linha \ref{alg:mod:re2}).

A atualização do modelo da option na proposta original, é precedida de um teste para evitar que a option correspondente ao evento saliente que possa ter ocorrido no passo atual, $o_e$, seja processada (linha \ref{alg:mod:oe:if}). Uma vantagem advinda disso é o de se poder utilizar os valores de $Q^{o_e}$ já atualizados. Entretanto, como a atualização de $Q^{o_e}$ somente ocorre ao final do algoritmo, é necessário esperar os próximos passos para utilizar os valores atualizados.  Dessa forma, e como a primeira referência a $o_e$ ocorre na linha \ref{alg:mod:oe:priocorr}, na nova proposta a inicialização e atribuição foram adicionadas nas linhas \ref{alg:mod:oe:ini} e \ref{alg:mod:oe:atr}, respectivamente. Isso foi feito por dois motivos: (i) definir $o_e$ de forma explícita no algoritmo e (ii) tratar os valores que $o_e$ assume de acordo com a lógica de atualização do modelo da option, aproveitando os valores atualizados de $Q^{o_e}$.

O algoritmo original prevê a adição de estados às options de forma retroativa: caso o agente, partindo de $s_t$ alcance $s_{t+1}$ e $s_{t+1} \in I^o$, $s_t$ é adicionado a $I^o$. Entretanto, caso uma option possa ser reiniciada em um estado terminal, pode ser gerado um ciclo de repetição indefinido em que a option chama a si mesma ao terminar. Dessa forma, na linha \ref{alg:mod:addI} foi adicionado um teste para evitar que um estado terminal da option seja adicionado ao seu \tr{conjunto iniciador}.

Como a escolha de quais ações são aplicáveis nesse momento é um fator importante na atualização do modelo das options, na proposta original, as funções $P^o$ e $R^o$ somente são atualizadas caso a ação $a_t$ executada no passo atual seja gulosa para a option em questão (linha \ref{alg:mod:atgreedy}). Isso pode ser explicado pelo fato de que a atualização dessas funções representem um consumo significativo de recursos computacionais. Na proposta atual, essa restrição foi investigada, sendo executados experimentos também para o caso em que não há a necessidade de $a_t$ ser a ação gulosa.

Na proposta original, a atualização de $Q_B$ para options (planejamento SMDP, linha \ref{alg:mod:qbo}) considerava para esse cálculo quaiquer estados $s_t$ que o agente viesse a visitar. Para evitar problemas de incosistência entre a função-valor e o \tr{conjunto iniciador} de uma option $o$, foi adicionado um teste que somente atualiza $Q_B(s_t,\ o)$ para uma dada option $o$ se $s_t \in I^o$. Dessa forma, evita-se que seja atribuído um valor positivo à execução de uma option $o$ em um estado em que a option não possa ser executada. Uma outra vantagem decorrente dessa modificação é a possível economia de recursos computacionais, já que algumas atualizações desnecessárias são potencialmente evitadas.

\begin{algorithm}
\caption{Algoritmo proposto (modificações sugeridas sublinhadas com pontilhado)}
\label{alg:modificado}
\algsetup{indent=2em}
\small
\begin{algorithmic}[1]
    \newcommand{\sd}{s_{t+1}}
    \newcommand{\bo}[1]{$\beta^o(#1)$}
    \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
    \newcommand{\tv}{\tr{valor terminal para a option $o$}}
    \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
    \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
    \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
    \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
    \newcommand{\termoA}{\bo{\sd}$v$}
    \newcommand{\termoB}{[\ubo{\sd}][\maos]}
    \newcommand{\termoC}{\bo{x}$v$}
    \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
    \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

    \REQUIRE{Inicializar $s_t,\ a_t,\ $\grifar{$o_t$}$,\ r^e_t,\ r^i_t$}

    \REPEAT

    \STATE Obter próximo estado $\sd$

    \STATE \label{alg:mod:re2} \grifar{$r^e_{t+1} \leftarrow$ recompensa extrínsica recebida na transição $s_t,\ a_t \rightarrow s_{t+1}$}

    \GIMMESPACE

    \STATE\COMMENT{Trata ocorrência de evento saliente}

    \STATE \label{alg:mod:oe:ini}\grifar{$o_e \leftarrow nil$}

    \IF{ocorreu um evento saliente $e$}

        \IF{option para $e$, $o_e$, não existe em $O$}\label{alg:mod:oe:priocorr}

            \STATE \label{alg:mod:oe:atr}\grifar{$o_e \leftarrow $[option para $e$]}

            \STATE Criar $o_e$ em $O$

            \STATE Adicionar $s_t$ a $I^{o_e}$

            \STATE $\beta^{o_e}(s_{t+1}) \leftarrow 1$

        \ENDIF

        \STATE $r^i_{t+1} \leftarrow \tau[1-P(s_{t+1}|s_t)]$ \COMMENT{$\tau$ constante}

    \ELSE

        \STATE $r^i_{t+1} \leftarrow 0$

    \ENDIF

    \GIMMESPACE

    \STATE\COMMENT{Atualiza o modelo das options}

    \FOR{cada option $o \neq $\grifar{$o_e$} em $O$} \label{alg:mod:oe:if}

        \IF{$s_{t+1} \in I^o$ \AND \grifar{$\beta^o(s_t) \neq 1$}} \label{alg:mod:addI}

            \STATE Adicionar $s_t$ a $I^o$

        \ENDIF

        \IF{\grifar{$a_t$ é a ação gulosa para $o$ no estado $s_t$}} \label{alg:mod:atgulosa}

            \STATE\COMMENT{Atualiza o modelo probabilístico de transição da option}

            \STATE $P(x|s_t) \xleftarrow{\alpha} [\gamma(1 - \beta^o(s_{t+1}))P^o(x|s_{t+1}) + \gamma\beta^o(s_{t+1})\delta_{s_{t+1}x}]$

            \STATE\COMMENT{Atualiza modelo de recompensa da option} \STATE $R^o(s_t) \xleftarrow{\alpha} [$\grifar{$r^e_{t+1}$}$ + \gamma(1 - \beta^o(s_{t+1}))R^o(s_{t+1})]$ \label{alg:mod:ind:model}

        \ENDIF

    \ENDFOR

    \GIMMESPACE

    \STATE\COMMENT{Atualização da \tr{função valor estado-ação} do Q-Learning}

    \STATE $Q_B(s_t,\ a_t) \xleftarrow{\alpha} $\grifar{$r^e_{t+1}$}$ + $\grifar{$r^i_{t+1}$}$ + \gamma\max_{a \in A \cup O}{Q_B(s_{t+1},\ a)}$ \label{alg:mod:ind:ql}

    \GIMMESPACE

    \STATE\COMMENT{Atualização da \tr{função valor estado-ação} para planejamento SMDP}

    \FOR{cada option $o$ em $O$}

        \IF{\grifar{$s_t \in I^o$}}

        \STATE $Q_B(s_t,\ o) \xleftarrow{\alpha} R^o(s_t) + \sum_{x \in S}{P^o(x|s_t)\max_{a \in A \cup O}Q_B(x,\ a)}$ \label{alg:mod:qbo}

        \ENDIF

    \ENDFOR

    \GIMMESPACE

    \STATE\COMMENT{Atualização das \tr{funções valor estado-ação} para as options}

    \FOR{cada option $o$ em $O$ tal que $s_t \in I^o$}

        \STATE $v \leftarrow$ \tv

        \STATE $Q^o(s_t,\ a_t)$\aalpha$\ $\grifar{$r^e_{t+1}$}$\ +\ \gamma\{$\termoA$\ +\ $\termoB$\}$ \label{alg:mod:ind:qo}

        \FOR{cada option $o'$ em $O$ tal que $s_t \in I^{o'}$ e $o \neq o'$}

            \STATE $Q^o(s_t,\ o')$\aalpha $R^{o'}(s_t)\ + \sum_{x \in S}{P^{o'}(x|s_t)}\{$\termoC$\ +\ $\termoD$\}$

        \ENDFOR

    \ENDFOR \label{alg:mod:re2:localantigo}

    \STATE Escolher $a_{t+1}$ através de estratégia \tr{$\epsilon$-gulosa} com respeito a $Q_B$

    \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}

    \UNTIL{condição de parada}

\end{algorithmic}
\end{algorithm}
	
\section{Resultados e Discussão}
% Checar localização desse parágrafo (seria um de arcabouço teórico?)
Espera-se como resultado da aplicação da utilização de Motivação Intrínseca em Aprendizado por Reforço um aceleramento do processo de aprendizado através da exploração da estrutura hierárquica do problema. Esse aproveitamento pode ser feito em três etapas: (i) identificação de \tr{sub-tarefas} e (ii) aprendizado e refinamento da solução das sub-tarefas (iii) composição da solução para a tarefa inicial utilizando as soluções das sub-tarefas.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{r_i-2014-07-12_07-15-03-Teri.eps}
    \caption{Evolução da recompensa intrínseca recebida no aprendizado}
    \label{fig:rixt}
\end{figure}

Para testar as propostas relacionadas ao algoritmo, procedeu-se à realização de experimentos , cuja função seria a de verificar empiricamente a validade da implementação. Essa verificação empírica serviria a dois propósitos: checar a consistência da implementação e avaliar a qualidade das modificações propostas.  Um experimento que se enquadra no primeiro dos propósitos consisitiu em avaliar a quantidade e valor das recompensas intrínsecas recebidas associadas a cada evento saliente possível (um resultado similar foi apresentado na Figura 4 do artigo original). O experimento consistiu de 100 repetições de $5\times10^5$ passos cada, onde o agente aprendia de acordo com o Algoritmo \ref{alg:modificado}. Os resultados estão presentes na figura \ref{fig:rixt}, onde são apresentados os valores de recompensa intrínseca recebidos pelo agente por passos do experimento.

A análise dos resultados presentes na Figura \ref{fig:rixt} sugere algumas características do aprendizado resultante: o agente inicialmente procede à execução das tarefas mais simples, como ligar/desligar a luz. Isso pode ser visto pela frequência e prematuridade com que o agente recebe recompensas intrínsecas para essas tarefas. Pode-se perceber uma variação dos valores recebidos associados ao evento saliente com a repetição dos mesmos: há uma tendência de \emph{diminuição} dos valores recebidos para eventos salientes repetidos.

É importante notar, entretanto, que, marcadamente no caso do evento \emph{lightON}, parece haver algumas vezes um \emph{aumento} da recompensa intrínseca recebida com a repetição. Isso pode ser devido à definição de evento saliente utilizada especificamente para esse experimento (somente atributos relacionados aos \tr{status} do ambiente), o que, associado à definição de $P^o$ para a descrição completa do estado, permitiria também uma evolução crescente de seus valores.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{imrlvsql.png}
    \caption{Comparação de aprendizado utilizando ou não motivação intrínseca para descoberta de options}
    \label{fig:imrlvsql}
\end{figure}

Pode-se também observar uma aparente \emph{graduação} do agente na execução das sub-tarefas: após o aprendizado das sub-tarefas mais simples, começam a ocorrer com mais frequência recompensas intrínsecas associadas a sub-tarefas mais complexas, que por sua vez são dependentes das mais simples. Em última instância, a sub-tarefa \emph{monkeyON} ocorre seguindo o padrão descrito acima e aproximadamente após o aprendizado das sub-tarefas mais simples de que \emph{monkeyON} depende. Essa é, das características citadas, a de maior interesse para este trabalho, pois sugere justamente a ocorrência de um aprendizado (e consequente utilização) das options de uma forma \emph{hierárquica}, partindo de sub-tarefas componentes mais simples até atinger o objetivo final do problema.

Para avaliar a importância relativa da descoberta de options com auxílio de motivação intrínseca, foi realizado um experimento no qual o agente aprende com o Q-Learning tradicional, ou seja, não utiliza recompensas intrínsecas para a descoberta de options e comparado com o desempenho do agente que utiliza o Algoritmo \ref{alg:modificado} para o aprendizado. O resultado desse experimento pode ser visto na figura \ref{fig:imrlvsql}, que mostra a quantidade média de passos necessárias para atingir o objetivo à medida que o agente o atinge, ou seja, quão rápido o agente consegue resolver a tarefa em \emph{cada vez menos passos}. Pela análise do gráfico, podemos ver que o agente que utiliza a motivação intrínseca além da extrínseca possui uma evolução mais rápida em relação ao agente que somente utiliza a motivação extrínseca, o que sugere que a vantagem da utilização da motivação intrínseca para obtenção de options no aprendizado.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{epsopt.png}
    \caption{Análise da sensibilidade do algoritmo à estratégia de abandono da option}
    \label{fig:epsopt}
\end{figure}

Para verificar a suscetibilidade do algoritmo proposto seria à \tr{estratégia de abandono da option}, foram realizadas repetições de experimentos com variações gradativas do valor de $\epsilon_o$, conforme descrito na seção \tr{...}. Os resultados são apresentados na figura \ref{fig:epsopt}. O gráfico mostra a média cumulativa de ativações do \emph{monkey} por passo. Pela análise desses resultados, podemos verificar a variação do desempenho do aprendizado com o valor escolhido para $\epsilon_o$, sugerindo que a correta escolha desse parâmetro possui um impacto significativo no desempenho de aprendizado do agente. \tr{TODO: complementar}

Foram também investigadas duas estratégias de adição de novos estados à option: (i) \tr{backward}, em que um novo estado $s_t$ somente é adicionado ao conjunto iniciador $I^o$ de uma option $o$ se ocorrer que ao realizar a transição $s_t,\ a_t \rightarrow s_{t+1}$, tenha-se $s_{t+1} \in I^o$ e (ii) \tr{backward+forward}, em que ocorre a adição descrita em (i), além de: caso o agente, estando em $s_t \in I^o$, realizar a transição  $s_t,\ a_t \rightarrow s_{t+1}$, $s_{t+1}$ é incluído em $I^o$. Como no algoritmo original, somente era realizada a adição (i), mas no artigo original de options \cite{option} é levantada também a possibilidade descrita em (ii), procedeu-se à investigação dos possíveis resultados decorrentes dessa nova adição. Os resultados desse experimento podem ser vistos na figura \ref{fig:dummy}, que mostra \tr{...}. Pela análise dos resultados aí presentes, podemos concluir que \tr{...}.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{dummy.png}
    \caption{\tr{TODO: consolidar e plotar} Comparação de aprendizado utilizando adições de estados à option pelas estratégias \tr{backward} e \tr{backward+forward}}
    \label{fig:dummy}
\end{figure}

\begin{received}
\end{received}

% INCLUDE BIBLIOGRAPHY WHICH MUST FOLLOW kdmile.bst TEMPLATE
\bibliographystyle{kdmile}
\bibliography{kdmileb}
% For information on how to write bibliography entries, 
% see file kdmileb.bib

\end{document}
