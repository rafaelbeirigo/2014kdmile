% by Mirella M. Moro; version: January/18/2012 @ 04:16pm
% -- 01/18/2012: more discussion on SBBD + kdmile; overall revision
% -- 09/03/2010: bib file with names for proceedings and journals; cls with shrinked {received}
% -- 08/27/2010: appendix, table example, more explanation within comments, editors' data

\documentclass[kdmile,a4paper]{kdmile} % NOTE: kdmile is published on A4 paper
\usepackage{graphicx,url}  % for using figures and url format
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[T1]{fontenc}   % avoids warnings such as "LaTeX Font Warning: Font shape 'OMS/cmtt/m/n' undefined"
\usepackage{amsmath}
\usepackage{bbm}
\usepackage[brazilian]{babel}
\usepackage[usenames,dvipsnames]{color}
\usepackage{soul,xcolor}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{amsmath}
\usepackage[normalem]{ulem}

%\usepackage{cite} % NOTE: do **not** include this package because it conflicts with kdmile.bst

% Standard definitions
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newdef{definition}[theorem]{Definition}
\newdef{remark}[theorem]{Remark}

% New environment definition
\newenvironment{latexcode}
{\ttfamily\vspace{0.1in}\setlength{\parindent}{18pt}}
{\vspace{0.1in}}

% Configurações gráficas
\DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg} % Preferência por .eps (maior resolução)
\graphicspath{{fig/}}                           % Pasta padrão para as figuras

% Traduções ref. packaged 'algorithm[ic]'
\floatname{algorithm}{Algoritmo} % termo utilizado na referenciação
\renewcommand{\algorithmicrequire}{\textbf{Requer:}}
\renewcommand{\algorithmicensure}{\textbf{Assegurar-se:}}
\renewcommand{\algorithmicend}{\textbf{fim}}
\renewcommand{\algorithmicif}{\textbf{se}}
\renewcommand{\algorithmicthen}{\textbf{então}}
\renewcommand{\algorithmicelse}{\textbf{senão}}
\renewcommand{\algorithmicelsif}{\algorithmicelse\ \algorithmicif}
\renewcommand{\algorithmicendif}{\algorithmicend\ \algorithmicif}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicforall}{\textbf{para todo}}
\renewcommand{\algorithmicdo}{\textbf{faça}}
\renewcommand{\algorithmicendfor}{\algorithmicend\ \algorithmicfor}
\renewcommand{\algorithmicwhile}{\textbf{enquanto}}
\renewcommand{\algorithmicendwhile}{\algorithmicend\ \algorithmicwhile}
\renewcommand{\algorithmicloop}{\textbf{loop}}
\renewcommand{\algorithmicendloop}{\algorithmicend\ \algorithmicloop}
\renewcommand{\algorithmicrepeat}{\textbf{repita}}
\renewcommand{\algorithmicuntil}{\textbf{até}}
\renewcommand{\algorithmicprint}{\textbf{imprima}}
\renewcommand{\algorithmicreturn}{\textbf{retorne}}
\renewcommand{\algorithmictrue}{\textbf{verdadeiro}}
\renewcommand{\algorithmicfalse}{\textbf{falso}}
\renewcommand{\algorithmicand}{\textbf{e}}
\renewcommand{\algorithmicor}{\textbf{ou}}

% Eventos salientes
\newcommand{\tmon}{$alarme_{ON}$}
\newcommand{\tmoff}{$alarme_{OFF}$}
\newcommand{\muon}{$musica_{ON}$}
\newcommand{\muoff}{$musica_{OFF}$}
\newcommand{\lion}{$luz_{ON}$}
\newcommand{\lioff}{$luz_{OFF}$}
\newcommand{\beon}{$sino_{ON}$}

% Termos/conceitos
\newcommand{\mi}{motivação intrínseca}
\newcommand{\epS}{\epsilon_{p}}
\newcommand{\ep}{$\epS$}
\newcommand{\pilhaS}{\mathcal{P}}
\newcommand{\pilha}{$\pilhaS$}
\newcommand{\attS}{a_{t+1}}
\newcommand{\att}{$\attS$}
\newcommand{\qbS}{Q^B}
\newcommand{\qb}{$\qbS$}
\newcommand{\peumS}{p_1}
\newcommand{\peum}{$\peumS$}
\newcommand{\td}{\emph{top-down}}
\newcommand{\bu}{\emph{bottom-up}}

% Comando para espaçamento vertical no ambiente algorithm
\newcommand{\GIMMESPACE}{\vspace{3.17 mm}}

% Para marcar palavras que precisam ser traduzidas
% \newcommand{\tr}[1]{\colorbox{MidnightBlue}{\textcolor{White}{\textbf{#1}}}}
\newcommand{\tr}[1]{\textcolor{MidnightBlue}{\textbf{#1}}}

% Para marcar partes do algoritmo original que foram modificadas
\sethlcolor{Green}
\newcommand{\grifar}[1]{\dotuline{#1}}
% \newcommand{\grifar}[1]{\hl{#1}}
% \newcommand{\grifar}[1]{\dashuline{#1}}
% \newcommand{\grifar}[1]{\uwave{#1}}

% Substituir pelo título quando estiver definido
\newcommand{\titulo}{Avaliação de Estratégias para Aplicação de\\ \ Políticas Parciais com Motivação Intrínseca}

%% Melhores comentários para o pacote algorithm
\renewcommand{\algorithmiccomment}[1]{// #1}

% ALL FIELDS UNTIL BEGIN{document} ARE MANDATORY


% Includes headers with simplified name of the authors and article title
\markboth{R. L. Beirigo and V. Freire and A. H. R. Costa}
{\titulo}
%  -> \markboth{}{}
%         takes 2 arguments
%         ex: \markboth{A.Plastino}{Any article title}


% Title of the article
\title{\titulo}


% List of authors
%IF THERE ARE TWO or more institutions, please use:
%\author{Name of Author1\inst{1}, Name of Author2\inst{2}, Name of Author3\inst{2}}
\author{R. L. Beirigo and V. Freire and A. H. R. Costa}

%Affiliation and email
\institute{Escola Politécnica da Universidade de São Paulo, Brazil \\
  \email{\{rafaelbeirigo, valdinei.freire, anna.reali\}@usp.br}
}

% Article abstract - it should be from 100 to 300 words
\begin{abstract}
  As técnicas de Aprendizado por Reforço permitem a solução de um
  problema através da escolha de ações que maximizem valores de
  recompensas recebidas que refletem a qualidade das ações tomadas
  pelo agente em um processo de tentativa e erro. Em problemas com
  estrutura hierárquica, a solução final depende do encadeamento de
  soluções para sub-problemas aí presentes, sendo frequente a
  repetição de sub-problemas nesse encadeamento. Nesses casos, a
  utilização de políticas parciais permite o aprendizado e
  armazenamento das soluções individuais para cada sub-problema, que
  podem então ser utilizadas múltiplas vezes na composição de uma
   solução completa para o problema final, acelerando o
  aprendizado. Apesar de vantajosa, a utilização de políticas parciais
  necessita de definições por parte do projetista, o que representa
  uma sobrecarga. Para contornar esse problema, foram propostas
  técnicas de descoberta automática de políticas parciais, dentre as
  quais a utilização de motivação intrínseca se destaca por permitir
  ao agente aprender soluções de sub-problemas úteis na solução do
  problema final sem a necessidade de se definir manualmente novas
  recompensas para esses sub-problemas individualmente. Apesar de
  promissora, essa proposta utiliza um conjunto de componentes de
  aprendizado que ainda carece de investigação aprofundada acerca dos
  impactos individual e coletivo de cada componente, notadamente a
  aplicação das políticas parciais durante o aprendizado. Neste
  trabalho são investigadas estratégias de aplicação de políticas
  parciais durante o Aprendizado por Reforço com Motivação
  Intrínseca. Duas estratégias são propostas e o desempenho de cada
  estratégia é avaliado experimentalmente. Com base nesta avaliação,
  uma nova estratégia é proposta, a qual utiliza as alternativas que
  apresentaram o melhor desempenho na avaliação experimental.
\end{abstract}

% ACM Computing Classification System categories
\category{I.2.6}{Artificial Intelligence}{Learning} 

% Categories and Descriptors are available at the 1998 ACM Computing Classification System
% http://www.acm.org/about/class/1998/
%  -> \category{}{}{}
%         takes 3 arguments for the Computing Reviews Classification Scheme.
%         ex: \category{D.3.3}{Programming Languages}{Language Constructs and Features}
%                   [data types and structures]
%                   the last argument, in square brackets, is optional.

% Article keywords
\keywords{aprendizado por reforço, descoberta de políticas parciais, motivação intrínseca}
%  -> \keywords{} (in alphabetical order \keywords{document processing, sequences,
%                      string searching, subsequences, substrings})

% THE ARTICLE BEGINS
\begin{document}

% This is optional:
\begin{bottomstuff}
% TODO: preencher adequadamente
% similar to \thanks
% for authors' addresses; research/grant statements
\end{bottomstuff}

\maketitle

\section{Introdução}
Problemas nos quais o agente deve tomar uma série de decisões
sequenciais podem ser resolvidos através de técnicas de Aprendizado
por Reforço \cite{rlczaba}, em que o agente recebe um reforço numérico
indicando a qualidade de cada ação tomada. Esse processo de
aprendizado ocorre por tentativa e erro, no qual o agente alterna
entre a \emph{aplicação} do conhecimento adquirido até o momento para
selecionar a melhor ação e a \emph{exploração} do ambiente, onde o
agente experimenta novas ações, aumentando o seu conhecimento sobre o
ambiente e o problema. Apesar do sucesso apresentado na aplicação de
técnicas AR a problemas reais
\cite{tesauro1995tdgammon,barto1996elevator,scardua2002optimal,da2006inverse,abbeel2007helicopter},
elas podem apresentar indesejada lentidão na obtenção das soluções
\cite{barto2003hrl}.
 
Uma forma de mitigar essa lentidão é explorar características da
\emph{representação} do problema original que permitam a aplicação
de técnicas de otimização. Um exemplo disso é a utilização de
políticas parciais \cite{option1999}, que permitem que um problema
de AR originalmente com representação monolítica seja representado
de forma modular. Na nova representação, são determinados
sub-problemas componentes do problema original, permitindo o
aprendizado simultâneo da solução para o problema original e seus
sub-problemas. Quando o problema original possui uma estrutura
\emph{hierárquica}, é comum o agente ter de resolver mais de uma vez
o mesmo sub-problema em momentos diferentes. Nesses casos, a
utilização de políticas parciais apresenta a vantagem de acelerar o
aprendizado por eliminar a necessidade de se aprender a solução para
o mesmo sub-problema mais de uma vez, permitindo o reuso do
conhecimento já adquirido na forma de políticas parciais
\cite{option1999} e dessa forma acelerando o aprendizado.

Entretanto, para que as políticas parciais possam ser utilizadas, há
a necessidade da definição de elementos cuja identificação pode não
ser imediata à apreciação do problema, podendo causar sobrecarga em
tempo de projeto. Para contornar esse revés, são estudadas técnicas
de descoberta automática de políticas parciais
\cite{stolle2002learning,bakker2004hierarchical,mcgovern2001automatic,imrl2004},
em que a definição das mesmas fica a cargo de um algoritmo
responsável por identificar sub-problemas e criar políticas parciais
para resolvê-los. Das diversas técnicas empregadas, pode ser
destacada a utilização de \emph{motivação intrínseca} em AR
\cite{imrl2004}, em que o agente recebe recompensas adicionais
através de um mecanismo interno associado a variações de
características do ambiente que lhe despertam \emph{marcado
interesse}. Nessas técnicas, o agente é codificado de forma a
associar sub-problemas a variações significativas de atributos
selecionados na descrição fatorada do estado, reduzindo o trabalho
de definição das políticas parciais à determinação dos atributos
relevantes para a identificação dos sub-problemas.

Essa técnica foi aplicada com sucesso na solução de um problema com
forte estrutura hierárquica \cite{imrl2004}, através da proposta de
um algoritmo que utiliza em conjunto múltiplas técnicas de
AR. Apesar da aprensentação detalhada do papel da motivação
intrínseca no aprendizado, não há uma investigação mais aprofundada
dos diversos componentes de aprendizado envolvidos algoritmo,
notadamente a estratégia de aplicação das políticas parciais
aprendidas. Neste trabalho, investigamos essa questão, apresentando
algumas propostas de estratégias para a aplicação das políticas
parciais aprendidas juntamente com a avaliação experimental de cada
uma delas e uma breve discussão teórica.

\section{Fundamentos Teóricos}
Problemas de decisão sequencial são usualmente representados por um
\emph{Processo Markoviano de Decisão} (PMD) \cite{puterman2009markov},
definido pela quádrupa $<S, A, P, R>$, em que $S$ e $A$ são conjuntos
finitos de estados e ações respectivamente,
$R:S\rightarrow\mathcal{R}$ representa a recompensa recebida ao
alcançar um estado $s \in S$ e $P:S\times A\times S \rightarrow [0,1]$
a probabilidade de alcançar um estado a partir de outro através da
execução de uma ação.  Uma política $\pi: S \rightarrow A$ associa uma
ação a ser executada em um estado e a função $V^{\pi}: S \rightarrow
\mathcal{R}$ fornece o valor esperado de recompensas futuras ao,
partindo de $s \in S$, executar $\pi$.  A solução de um PDM ṕé dado
pela política \emph{ótima} $\pi^* = \arg$
que indica pode ser obtida por técnicas de Aprendizado por Reforço
(AR) \cite{rlczaba}

Um problema de decisão sequencial modelado por um Markovian Decision
Process (MDP) (Puterman, 2009) é definido pela quádrupla , na qual


compreende um conjunto de estados do ambiente, contém as ações
possíveis de serem executadas em cada estado, ambos finitos, e
representam, respectivamente, a probabilidade de transição e a
recompensa recebida ao, partindo de um estado alcançar o estado
executando-se a ação .

Uma política  associa a cada par  uma probabilidade de se executar a
ação  estando estado , sendo o valor de uma política  obtido através
da função , onde  fornece o valor esperado de recompensas futuras ao
se seguir a política  partindo do estado .
A solução para um MDP consiste em uma política ótima , tal que , ou
seja,  maximiza o valor esperado de recompensas recebidas para todos
os estados aí presentes.
Métodos de RL (Sutton e Barto, 1998) podem ser utilizados para
solucionar MDPs. Ao utilizar uma técnica de RL, um agente interage com
o ambiente, realizando uma ação para cada estado e observando a
recompensa recebida. O intuito do agente é o de maximizar o valor
esperado de recompensas recebidas.
Para isso, o agente necessita experimentar ações em estados ainda não
visitados através de exploração, mas deve também guiar sua trajetória
utilizando o conhecimento já adquirido sobre os estados visitados
anteriormente.
Dessa forma, há um compromisso entre a descoberta de informação sobre
o ambiente e a utilização do conhecimento adquirido até o momento,
muitas vezes referido como o compromisso exploração/explotação.
/
\section{Técnica Proposta}\label{sec:algoritmo}
Para explorar a característica hierárquica presente nas políticas
parciais de forma a acelerar o aprendizado, o algoritmo presente em
\cite{imrl2004} foi adaptado para disponibilizar ao agente uma
estrutura \pilha\ em forma de pilha para armazenar a hierarquia de
políticas parciais utilizadas pelo agente.  No momento da obtenção da
próxima ação, esta pode ser executada de forma aleatória ou gulosa (na
aplicação de \qb\ ou de uma política parcial).  Em cada um desses
momentos, caso a próxima ação selecionada seja uma política parcial
$p_1$, esta é adicionada ao topo da pilha.  Dessa forma, um agente que
inicia com uma pilha vazia $\pilhaS=[\ ]$ e obtém como próxima ação a
política parcial \peum\ resultará com $\pilhaS=[\peumS]$. Caso a
aplicação de \peum\ sugira a execução de uma nova política parcial
$p_2$, essa será então empilhada, resultando em $\pilhaS=[\peumS,
p_2]$ e assim sucessivamente para novas políticas parciais.

Uma política parcial é removida da pilha no momento em que seu
objetivo é atingido pelo agente, ou seja, o evento saliente utilizado
para criá-la é disparado.  Como uma política parcial $p$ pode invocar
outras de modo a atingir seu objetivo, $p$ é removida da pilha
juntamente com as políticas parciais invocadas por ela.  A estratégia
de remoção testa as políticas parciais para remoção partindo da base
da pilha em direção ao topo, ou seja, prioriza a remoção das políticas
parciais mais anteriormente adicinadas.

Propõe-se também um mecanismo de \emph{exploração interna} à política
parcial, elaborada para atenuar o efeito de \emph{rigidez}
potencialmente imposto pela política parcial, em que o agente fica
preso a uma sequência de ações. Dessa forma, durante a aplicação de
uma política parcial por parte do agente, no momento da escolha da
próxima ação a ser executada, originalmente essa escolha seria feita
sempre de forma gulosa somente.  Isso foi feito porque, apesar de a
estratégia gulosa ser a melhor no caso de a política parcial ser ótima
para o sub-problema sendo resolvido, ela implica em uma restrição que
potencialmente atrapalharia o desempenho de aprendizado do agente nos
passos iniciais, em que as políticas parciais estão em fase inicial de
aprendizado.  Dessa forma, o parâmetro $\epS \in [0,1]$ indica o
percentual de exploração utilizado na aplicação da política parcial.
No momento da aplicação da política parcial, é sorteado um valor no
intervalo $[0,1]$; caso esse valor seja maior que \ep\, o agente
aplica a política parcial sendo seguida de forma gulosa e, em caso
contrário, o agente realiza exploração, ou seja, executa uma ação
aleatória dentre as possíveis para o estado atual.

% No Algoritmo \ref{alg:modificado} encontram-se marcadas as
% modificações propostas em relação ao algoritmo original
% \cite{imrl2004}. O intuito é o de contribuir à proposta original
% soluções de eventuais ambiguidades ou aparentes inconsistências,
% dessa forma apresentando um algoritmo que, mantendo o espírito
% original da utilização de Motivação Instrínseca no Aprendizado por
% Reforço, acrescenta alternativas de implementação buscando obter
% melhorias de desempenho no aprendizado. Abaixo são descritas as
% motivações e respectivas motivações teóricas e/ou de cunho prático.

% Nas linhas \ref{alg:mod:ind:model}, \ref{alg:mod:ind:ql} e
% \ref{alg:mod:ind:qo}, os índices de $r^e_t$ e $r^i_t$ foram
% modificados para $r^e_{t+1}$ e $r^i_{t+1}$, respectivamente. Isso
% foi feito para permitir que as atualizações que utilizam esses
% valores considerassem a transição $s_t,\ a_t \leftarrow s_{t+1}$ ao
% invés da precedente a esta, como era anteriormente. Ainda de acordo
% com essa modificação, a obtenção de $r^e_{t+1}$, que anteriormente
% era realizada após a linha \ref{alg:mod:re2:localantigo}, passa a
% ser realizada logo após a transição de estados (linha
% \ref{alg:mod:re2}).

% A atualização do modelo da política parcial na proposta original, é
% precedida de um teste para evitar que a política parcial
% correspondente ao evento saliente que possa ter ocorrido no passo
% atual, $o_e$, seja processada (linha \ref{alg:mod:oe:if}). Uma
% vantagem advinda disso é o de se poder utilizar os valores de
% $Q^{o_e}$ já atualizados. Entretanto, como a atualização de
% $Q^{o_e}$ somente ocorre ao final do algoritmo, é necessário esperar
% os próximos passos para utilizar os valores atualizados. Dessa
% forma, e como a primeira referência a $o_e$ ocorre na linha
% \ref{alg:mod:oe:priocorr}, na nova proposta a inicialização e
% atribuição foram adicionadas nas linhas \ref{alg:mod:oe:ini} e
% \ref{alg:mod:oe:atr}, respectivamente. Isso foi feito por dois
% motivos: (i) definir $o_e$ de forma explícita no algoritmo e (ii)
% tratar os valores que $o_e$ assume de acordo com a lógica de
% atualização do modelo da política parcial, aproveitando os valores
% atualizados de $Q^{o_e}$.

% O algoritmo original prevê a adição de estados às políticas parciais
% de forma retroativa: caso o agente, partindo de $s_t$ alcance
% $s_{t+1}$ e $s_{t+1} \in I^o$, $s_t$ é adicionado a
% $I^o$. Entretanto, caso uma política parcial possa ser reiniciada em
% um estado terminal, pode ser gerado um ciclo de repetição indefinido
% em que a política parcial chama a si mesma ao terminar. Dessa forma,
% na linha \ref{alg:mod:addI} foi adicionado um teste para evitar que
% um estado terminal da política parcial seja adicionado ao seu
% conjunto iniciador.

% Como a escolha de quais ações são aplicáveis nesse momento é um
% fator importante na atualização do modelo das políticas parciais, na
% proposta original, as funções $P^o$ e $R^o$ somente são atualizadas
% caso a ação $a_t$ executada no passo atual seja gulosa para a
% política parcial em questão (linha \ref{alg:mod:atgreedy}). Isso
% pode ser explicado pelo fato de que a atualização dessas funções
% representem um consumo significativo de recursos computacionais. Na
% proposta atual, essa restrição foi investigada, sendo executados
% experimentos também para o caso em que não há a necessidade de $a_t$
% ser a ação gulosa.

% Na proposta original, a atualização de $Q_B$ para políticas parciais
% (planejamento SMDP, linha \ref{alg:mod:qbo}) considerava para esse
% cálculo quaiquer estados $s_t$ que o agente viesse a visitar. Para
% evitar problemas de incosistência entre a função-valor e o conjunto
% iniciador de uma política parcial $o$, foi adicionado um teste que
% somente atualiza $Q_B(s_t,\ o)$ para uma dada política parcial $o$
% se $s_t \in I^o$. Dessa forma, evita-se que seja atribuído um valor
% positivo à execução de uma política parcial $o$ em um estado em que
% a política parcial não possa ser executada. Uma outra vantagem
% decorrente dessa modificação é a possível economia de recursos
% computacionais, já que algumas atualizações desnecessárias são
% potencialmente evitadas.

% \begin{algorithm}
% \caption{Algoritmo X}
% \label{alg:modificado}
% \algsetup{indent=2em}
% \small
% \begin{algorithmic}[1]
%     \newcommand{\sd}{s_{t+1}}
%     \newcommand{\bo}[1]{$\beta^o(#1)$}
%     \newcommand{\ubo}[1]{$1\ -\ $\bo{#1}}
%     \newcommand{\tv}{\tr{valor terminal para a política parcial $o$}}
%     \newcommand{\maxao}[1]{$\max_{a \in A \cup O}{#1}$}
%     \newcommand{\maos}{\maxao{Q^o(\sd,\ a)}}
%     \newcommand{\maox}{\maxao{Q^o(x,\ a)}}
%     \newcommand{\aalpha}{$\xleftarrow{\alpha}$}
%     \newcommand{\termoA}{\bo{\sd}$v$}
%     \newcommand{\termoB}{[\ubo{\sd}][\maos]}
%     \newcommand{\termoC}{\bo{x}$v$}
%     \newcommand{\termoD}{$[$\ubo{x}$][$\maox$]$}
%     \newcommand{\transita}[1]{$#1_t \leftarrow #1_{t+1}$}

%     \REQUIRE{Inicializar $s_t,\ a_t,\ $\grifar{$o_t$}$,\ r^e_t,\ r^i_t$}

%     \REPEAT

%     \STATE Obter próximo estado $\sd$

%     \STATE \label{alg:mod:re2} \grifar{$r^e_{t+1} \leftarrow$ recompensa extrínsica recebida na transição $s_t,\ a_t \rightarrow s_{t+1}$}

%     \GIMMESPACE

%     \STATE\COMMENT{Trata ocorrência de evento saliente}

%     \STATE \label{alg:mod:oe:ini}\grifar{$o_e \leftarrow nil$}

%     \IF{ocorreu um evento saliente $e$}

%         \IF{política parcial para $e$, $o_e$, não existe em $O$}\label{alg:mod:oe:priocorr}

%             \STATE \label{alg:mod:oe:atr}\grifar{$o_e \leftarrow $[política parcial para $e$]}

%             \STATE Criar $o_e$ em $O$

%             \STATE Adicionar $s_t$ a $I^{o_e}$

%             \STATE $\beta^{o_e}(s_{t+1}) \leftarrow 1$

%         \ENDIF

%         \STATE $r^i_{t+1} \leftarrow \tau[1-P(s_{t+1}|s_t)]$ \COMMENT{$\tau$ constante}

%     \ELSE

%         \STATE $r^i_{t+1} \leftarrow 0$

%     \ENDIF

%     \GIMMESPACE

%     \STATE\COMMENT{Atualiza o modelo das políticas parciais}

%     \FOR{cada política parcial $o \neq $\grifar{$o_e$} em $O$} \label{alg:mod:oe:if}

%         \IF{$s_{t+1} \in I^o$ \AND \grifar{$\beta^o(s_t) \neq 1$}} \label{alg:mod:addI}

%             \STATE Adicionar $s_t$ a $I^o$

%         \ENDIF

%         \IF{\grifar{$a_t$ é a ação gulosa para $o$ no estado $s_t$}} \label{alg:mod:atgulosa}

%             \STATE\COMMENT{Atualiza o modelo probabilístico de transição da política parcial}

%             \STATE $P(x|s_t) \xleftarrow{\alpha} [\gamma(1 - \beta^o(s_{t+1}))P^o(x|s_{t+1}) + \gamma\beta^o(s_{t+1})\delta_{s_{t+1}x}]$

%             \STATE\COMMENT{Atualiza modelo de recompensa da política parcial} \STATE $R^o(s_t) \xleftarrow{\alpha} [$\grifar{$r^e_{t+1}$}$ + \gamma(1 - \beta^o(s_{t+1}))R^o(s_{t+1})]$ \label{alg:mod:ind:model}

%         \ENDIF

%     \ENDFOR

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização da \tr{função valor estado-ação} do Q-Learning}

%     \STATE $Q_B(s_t,\ a_t) \xleftarrow{\alpha} $\grifar{$r^e_{t+1}$}$ + $\grifar{$r^i_{t+1}$}$ + \gamma\max_{a \in A \cup O}{Q_B(s_{t+1},\ a)}$ \label{alg:mod:ind:ql}

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização da \tr{função valor estado-ação} para planejamento SMDP}

%     \FOR{cada política parcial $o$ em $O$}

%         \IF{\grifar{$s_t \in I^o$}}

%         \STATE $Q_B(s_t,\ o) \xleftarrow{\alpha} R^o(s_t) + \sum_{x \in S}{P^o(x|s_t)\max_{a \in A \cup O}Q_B(x,\ a)}$ \label{alg:mod:qbo}

%         \ENDIF

%     \ENDFOR

%     \GIMMESPACE

%     \STATE\COMMENT{Atualização das \tr{funções valor estado-ação} para as políticas parciais}

%     \FOR{cada política parcial $o$ em $O$ tal que $s_t \in I^o$}

%         \STATE $v \leftarrow$ \tv

%         \STATE $Q^o(s_t,\ a_t)$\aalpha$\ $\grifar{$r^e_{t+1}$}$\ +\ \gamma\{$\termoA$\ +\ $\termoB$\}$ \label{alg:mod:ind:qo}

%         \FOR{cada política parcial $o'$ em $O$ tal que $s_t \in I^{o'}$ e $o \neq o'$}

%             \STATE $Q^o(s_t,\ o')$\aalpha $R^{o'}(s_t)\ + \sum_{x \in S}{P^{o'}(x|s_t)}\{$\termoC$\ +\ $\termoD$\}$

%         \ENDFOR

%     \ENDFOR \label{alg:mod:re2:localantigo}

%     \STATE Escolher $a_{t+1}$ através de estratégia \tr{$\epsilon$-gulosa} com respeito a $Q_B$

%     \STATE \transita{s}; \transita{a}; \transita{r^e}; \transita{r^i}

%     \UNTIL{condição de parada}

% \end{algorithmic}
% \end{algorithm}

\section{Experimentos}
O domínio utilizado na avaliação experimental foi o \emph{Playroom}
\cite{imrl2004}, em que o agente se econtra em um reticulado
$5\times5$ e deve descobrir como ativar um \emph{alarme} interagindo
com objetos aí presentes.
O agente é composto de um \emph{olho}, uma \emph{mão} e um \emph{alvo}
e possui sempre disponíveis as seguintes ações: apontar \emph{olho}
para a célula onde se encontra a \emph{mão} ou para a célula onde se
encontra o \emph{alvo}; mover a \emph{mão} ou o \emph{alvo} para a
célula atualmente apontada pelo \emph{olho}.
\begin{wrapfigure}{r}{0.5\textwidth}
  \small
  \begin{center}
    % BEGIN RECEIVE ORGTBL thenameofthetable
\begin{tabular}{llll}
\hline
Objeto & Ação & Evento & Mov. \\
 &  & saliente &  \\
\hline
alarme & - & \tmon\ & - \\
 & - & \tmoff\ & - \\
botão A & pressionar & \muon\ & - \\
 & empurrar & - & V \\
botão B & pressionar & \muoff\ & - \\
 & empurrar & - & V \\
bola & chutar &  & A \\
interruptor & pressionar & \lion & - \\
 & pressionar & \lioff & - \\
sino & - & \beon\ & V \\
\hline
\end{tabular}
    % END RECEIVE ORGTBL thenameofthetable
  \end{center}
  \tiny
  \caption{Objetos presentes no domínio com as correspondentes ações e eventos salientes}
  \label{fig:obj}
\end{wrapfigure}

Na tabela ao lado estão relacionados os objetos aí presentes,
juntamente com as ações e o evento saliente que disparam.  Essas ações
ficam disponíveis somente quando o \emph{olho} e a \emph{mão} estão
sobre a célula em que se encontra o objeto.  Na coluna \emph{Mov.},
objetos marcados com \emph{A} movem-se para a posição atual do
\emph{alvo} e as marcadas com \emph{V} para uma célula vizinha
aleatória ao serem ativados.  Caso a \emph{bola} seja chutada com o
\emph{alvo} sobre o \emph{sino}, este é ativado, disparando o evento
\beon.  O \emph{alarme} dispara quando o ocorre \beon\ a luz está
ligada e a música desligada simultaneamente e é desativado quando o
estado da luz ou da música é modificado; o agente somente consegue
diferenciar os botões e agir sobre eles se a luz estiver ligada.

Para avaliar a técnica proposta, foram realizados dois experimentos,
sendo que em um deles o módulo de obtenção de políticas parciais a
partir de \mi\ foi desativado, ou seja, o agente somente receberia
recompensas extrínsecas, sem a obtenção de políticas parciais.  Foram
realizados 100 experimentos, de $5e5$ passos cada, com o agente
recebendo uma recompensa extrínseca para ocorrência de \tmon\ no valor
de 10 e recompensas intrínsecas para os demais eventos salientes no
valor de 1.  Os resultados são apresentados no gráfico da Figura
\ref{fig:mi}, que apresenta a média de passos necessários para ativar
\tmon\ por quantidade de ativação. Os resultados aí presentes sugerem
maior eficiência de aprendizado por parte do agente que utiliza \mi\,
pois este diminui o tempo necessário para a solução do problema com
uma maior rapidez em relação ao agente que utiliza somente motivação
extrínseca.

No gráfico da Figura \ref{fig:mi} são apresentados os valores de
recompensa intrínseca recebida por passo do experimento, evidenciando
algumas características do aprendizado utilizando \mi : o agente
inicialmente procede à execução das tarefas mais simples, como
ligar/desligar a luz, o que pode ser visto pela frequência e
prematuridade com que o agente recebe recompensas intrínsecas para
essas tarefas. Pode-se perceber também uma variação dos valores
recebidos associados ao evento saliente com a repetição dos mesmos: há
uma tendência de \emph{diminuição} dos valores recebidos para eventos
salientes repetidos, mostrando a capacidade da técnica de priorizar a
realização de tarefas menos conhecidas.

Para avaliar o impacto da utilização de exploração interna à política
parcial, foram realizados 100 experimentos de 500 mil passos cada com
a mesma parametrização de recompensas anterior e com valores de $\epS
\in \{0.2, 0.4, 0.6, 0.8\}$.  O desempenho foi comparado com um agente
utilizando Q-Learning e outro com comportamento aleatório e os
resultados estão presentes na Figura \ref{fig:ep}, que apresenta a
média cumulativa de ativações do \tmon\ por passo de experimento.  A
análise do gráfico sugere que a utilização da técnica proposta para
obtenção e aplicação de políticas parciais pode superar a utilzação de
algoritmos que não utilizam políticas parciais.  Pode-se também
perceber que o valor de \ep\ possui impacto significativo no
aprendizado, sendo o desempenho apresentado diretamente proporcional
ao valor de \ep.

\begin{figure}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
      \includegraphics[width=\textwidth]{EfeitoMI}
      \caption{Efeito da Motivação Intrínseca no Aprendizado}
      \label{fig:mi}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.47\textwidth}
      \includegraphics[width=\textwidth]{toyonxstep-mediaacum-epsopt-parcial-02-04-06-08}
      \caption{Influência de \ep\ no Aprendizado}
      \label{fig:ep}
    \end{subfigure}
  \end{subfigure}

  \begin{subfigure}[t]{\textwidth}
    \includegraphics[width=\textwidth]{r_i-2014-07-12_07-15-03-Teri}
    \caption{Recebimento de Recompensa Intrínseca Durante o Aprendizado}
    \label{fig:ri}
  \end{subfigure}
  \caption{}\label{fig:res}
\end{figure}

% \section{Conclusão}
% Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Donec
% hendrerit tempor tellus. Donec pretium posuere tellus. Proin quam
% nisl, tincidunt et, mattis eget, convallis nec, purus. Cum sociis
% natoque penatibus et magnis dis parturient montes, nascetur
% ridiculus mus. Nulla posuere. Donec vitae dolor. Nullam tristique
% diam non turpis. Cras placerat accumsan nulla. Nullam rutrum. Nam
% vestibulum accumsan nisl.

% \begin{received}
% \end{received}

% INCLUDE BIBLIOGRAPHY WHICH MUST FOLLOW kdmile.bst TEMPLATE
\bibliographystyle{kdmile}
\bibliography{kdmile}
% For information on how to write bibliography entries, 
% see file kdmileb.bib

\end{document}

#+ORGTBL: SEND thenameofthetable orgtbl-to-latex :splice nil :skip 0
|-------------+------------+----------+------|
| Objeto      | Ação       | Evento   | Mov. |
|             |            | saliente |      |
|-------------+------------+----------+------|
| alarme      | -          | \tmon\   | -    |
|             | -          | \tmoff\  | -    |
| botão A     | pressionar | \muon\   | -    |
|             | empurrar   | -        | V    |
| botão B     | pressionar | \muoff\  | -    |
|             | empurrar   | -        | V    |
| bola        | chutar     |          | A    |
| interruptor | pressionar | \lion    | -    |
|             | pressionar | \lioff   | -    |
| sino        | -          | \beon\   | V    |
|-------------+------------+----------+------|
